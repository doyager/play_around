Index:

Writable and WritableComparable in Hadoop



#hive vs hbase:


where should be avro used?
dataframe vs dataset?


what happens when you upload a file to hdfs ? walkthrough fsimage edits etc
managed v unmanaged table 
secondary namenode vs stand by namenode 
managed table vs unmanaged talbe 
how spark supports puthon  , how psakr understand python given it is a jvm kind of Language
apply vs unapply in scala 
kafka offset management ,how does it happen ?
rebalancing kafka 
diff between cache and presist spark 
hadoop journel
hadoop fsimage and edit log 
who manages fsimage
catalyst optimizer - spark ??
advantage of case class in scala 


kafka - offset managment
kafka - paralleslism vs concurrancy
auxiliary constructor
casaa=ndra nosql
BloomMapFile
pig - physical plan vs logical plan

42.What is BloomMapFile used for? 
The BloomMapFile is a class that extends MapFile. So its functionality is similar to MapFile. BloomMapFile uses dynamic Bloom filters to provide quick membership test for the keys. It is used in Hbase table format.

 When is it a good idea to set Spark locality wait to zero?
https://support.datastax.com/hc/en-us/articles/208237373-FAQ-When-is-it-a-good-idea-to-set-Spark-locality-wait-to-zero-
.setConf("spark.task.maxFailures", "2")
              .setConf("spark.kryoserializer.buffer", "1024")
              .setConf("spark.kryoserializer.buffer.max", "2047")
              .setConf("spark.yarn.maxAppAttempts", "1")
              .setConf("spark.schedular.mode", "FAIR")
              .setConf("spark.locality.wait", "1m")


.setConf("spark.dynamicAllocation.enabled", "true")





#spark 1.6 vs spark 2




# rdd vs dataframe vs dataset 



# hive

#list set commands

#sqoop over write hive 

that --hive-overwrite does not delete existing dir but it overwrites the HDFS data location of hiv

sqoop import --connect jdbc:mysql://localhost/test --username root --password 'hr' --table sample --hive-import --hive-overwrite --hive-table sqoophive -m 1 --fields-terminated-by '\t' --lines-terminated-by '\n'

#RDD Storage levels:

			Storage Level

			Meaning

			MEMORY_ONLY

			Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.

			MEMORY_AND_DISK

			Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.

			MEMORY_ONLY_SER

			Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.

			MEMORY_AND_DISK_SER

			Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.

			DISK_ONLY

			Store the RDD partitions only on disk.

			MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.

			Same as the levels above, but replicate each partition on two cluster nodes.

			OFF_HEAP (experimental)

			Store RDD in serialized format in Tachyon. Compared to MEMORY_ONLY_SER, OFF_HEAP reduces garbage collection overhead and allows executors to be smaller and to share a pool of memory, making it attractive in environments with large heaps or multiple concurrent applications. Furthermore, as the RDDs reside in Tachyon, the crash of an executor does not lead to losing the in-memory cache. In this mode, the memory in Tachyon is discardable. Thus, Tachyon does not attempt to reconstruct a block that it evicts from memory.



#hadoop archive
https://community.cloudera.com/t5/Community-Articles/How-HAR-Hadoop-Archive-works/ta-p/249141

How HAR ( Hadoop Archive ) works

Hadoop archives is one of the methodology which is followed to reduce the load on the Namenode by archiving the files and referring all the archives as a single file via har reader.

hadoop archive -archiveName <value> <src> <dsstpath>
Execute hadoop archive commands
sudo -u hdfs hadoop archive -archiveName hartest2.har -p /tmp harSourceFolder2 /tmp/harDestinationFolder2

Hadoop archives are special format archives. A Hadoop archive maps to a file system directory. A Hadoop archive always has a *.har extension. A Hadoop archive directory contains metadata (in the form of _index and _masterindex) and data (part-*) files. The _index file contains the name of the files that are part of the archive and the location within the part files.

How to Create an Archive
Usage: hadoop archive -archiveName name -p <parent> <src>* <dest>



#Hadoop jar commands:

-files <comma separated list of files>	
Specify comma separated files to be copied to the map reduce cluster. Applies only to job.

-libjars <comma seperated list of jars>	Specify
comma separated jar files to include in the classpath. Applies only to job.

-archives <comma separated list of archives>	
Specify comma separated archives to be unarchived on the compute machines. Applies only to job.


# val vs var vs lazy val

“val” is used to define Immutable data. It’s evaluated only once at the time of definition.
“var” is used to define Mutable data. It’s evaluated only once at the time of definition.
Both val and var are evaluated Eagerly.
“lazy val” is used to define Immutable data. It is evaluated only once when we access it for first time. That means it is evaluated Lazily.
“def” is used to define Methods or Functions. It is evaluated only when we access it and evaluated every-time we access it. That means it is evaluated Lazily.




# map vs map partitions vs flat map

Map():

It returns a new RDD by applying the given function to each element of the RDD. 
The function in the map returns only one item.

MapPartitions():

Similar to map, but runs separately on each partition (block) of the RDD, so the function must be of type Iterator<T> ⇒ Iterator<U> when running on an RDD of type T.

flatMap() 
as compared to Map() and MapPartitions(), flatMap() neither works on a single element as map() nor it produces multiple elements of the result as mapPartitions().

Let me explain you with an example. If there are 2000 row and 20 partitions, then each partition will contain the 2000/20=100 Rows.

Now, when we apply map(func) method to rdd, the func() operation will be applied on each and every Row and in this particular case func() operation will be called 2000 times. i.e. time-consuming in some time-critical applications.

If we call mapPartition(func) method on rdd, the func() operation will be called on each partition instead of each row. In this particular case, it will be called 20 times(number of the partition). In this way, you can prevent some processing when it comes to time-consuming application.

Map() exercises function at per element level whereas MapPartitions() exercises function at the partition level.

Now talking about similarity of flatMap() as compared to Map() and MapPartitions(), flatMap() neither works on a single element as map() nor it produces multiple elements of the result as mapPartitions().


# fold vs foldleft vs foldright

Now, the difference between fold, foldLeft, and foldRight.
The primary difference is the order in which the fold operation iterates through the collection in question. foldLeft starts on the left side—the first item—and iterates to the right; foldRight starts on the right side—the last item—and iterates to the left. fold goes in no particular order.

Because fold does not go in any particular order, there are constraints on the start value and thus return value (in all three folds the type of the start value must be the same as the return value).

The first constraint is that the start value must be a supertype of the object you're folding. In our first example we were folding on a type List[Int] and had a start type of Int. Int is a supertype of List[Int].

The second constraint of fold is that the start value must be neutral, i.e. it must not change the result. For example, the neutral value for an addition operation would be 0, 1 for multiplication, Nil lists, etc.



#Spark Garbage Collection Tuning

In garbage collection, tuning in Apache Spark, the first step is to gather statistics on how frequently garbage collection occurs. It also gathers the amount of time spent in garbage collection. Thus, can be achieved by adding -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps to Java option. The next time when Spark job run, a message will display in workers log whenever garbage collection occurs. These logs will be in worker node, not on drivers program.

Java heap space divides into two regions Young and Old. The young generation holds short-lived objects while Old generation holds objects with longer life. The garbage collection tuning aims at, long-lived RDDs in the old generation. It also aims at the size of a young generation which is enough to store short-lived objects. With this, we can avoid full garbage collection to gather temporary object created during task execution. Some steps that may help to achieve this are:

If full garbage collection is invoked several times before a task is complete this ensures that there is not enough memory to execute the task.
In garbage collection statistics, if OldGen is near to full we can reduce the amount of memory used for caching. This can be achieved by lowering spark.memory.fraction. the better choice is to cache fewer objects than to slow down task execution. Or we can decrease the size of young generation i.e., lowering –Xmn.
The effect of Apache Spark garbage collection tuning depends on our application and amount of memory used.


# mapreduce combiner 

                  A Combiner, also known as a semi-reducer, is an optional class that operates by accepting the inputs from the Map class and thereafter passing the output key-value pairs to the Reducer class.

                  The main function of a Combiner is to summarize the map output records with the same key. The output (key-value collection) of the combiner will be sent over the network to the actual Reducer task as input.

                  Combiner
                  The Combiner class is used in between the Map class and the Reduce class to reduce the volume of data transfer between Map and Reduce. Usually, the output of the map task is large and the data transferred to the reduce task is high.

                  Here is a brief summary on how MapReduce Combiner works −

                  A combiner does not have a predefined interface and it must implement the Reducer interface’s reduce() method.

                  A combiner operates on each map output key. It must have the same output key-value types as the Reducer class.

                  A combiner can produce summary information from a large dataset because it replaces the original Map output.

                  Although, Combiner is optional yet it helps segregating data into multiple groups for Reduce phase, which makes it easier to process

# msckrepari - msck repair - metastore check command

          However, users can run a metastore check command with the repair table option:
          -which will update metadata about partitions to the Hive metastore for partitions for which such metadata doesn't already exist. 
          - The default option for MSC command is ADD PARTITIONS. With this option, it will add any partitions that exist on HDFS but not in metastore to the metastore.
          - The DROP PARTITIONS option will remove the partition information from metastore, that is already removed from HDFS. 
          - The SYNC PARTITIONS option is equivalent to calling both ADD and DROP PARTITIONS

           When there is a large number of untracked partitions, there is a provision to run MSCK REPAIR TABLE batch wise to avoid OOME (Out of Memory Error). By giving the configured batch size for the property hive.msck.repair.batch.size it can run in the batches internally. The default value of the property is zero, it means it will execute all the partitions at once. MSCK command without the REPAIR option can be used to find details about metadata mismatch metastore.

          The equivalent command on Amazon Elastic MapReduce (EMR)'s version of Hive is:

          ALTER TABLE table_name RECOVER PARTITIONS;

          MSCK [REPAIR] TABLE table_name [ADD/DROP/SYNC PARTITIONS];

          Recover Partitions (MSCK REPAIR TABLE)
          Hive stores a list of partitions for each table in its metastore. If, however, new partitions are directly added to HDFS (say by using hadoop fs -put command) or removed from HDFS, the metastore (and hence Hive) will not be aware of these changes to partition information unless the user runs ALTER TABLE table_name ADD/DROP PARTITION commands on each of the newly added or removed partitions, respectively.




# groubykey vs reduce by key 

https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html

    ->Avoid GroupByKey
        Let's look at two different ways to compute word counts, one using reduceByKey and the other using groupByKey:

        val words = Array("one", "two", "two", "three", "three", "three")
        val wordPairsRDD = sc.parallelize(words).map(word => (word, 1))

        Code:
                        val wordCountsWithReduce = wordPairsRDD
                          .reduceByKey(_ + _)
                          .collect()

                        val wordCountsWithGroup = wordPairsRDD
                          .groupByKey()
                          .map(t => (t._1, t._2.sum))
                          .collect()
                  
                  
->While both of these functions will produce the correct answer, 
->the reduceByKey example works much better on a large dataset. 
That's because Spark knows it can combine output with a common key on each partition before shuffling the data.

Look at the diagram below to understand what happens with reduceByKey. Notice how pairs on the same machine with the same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled. Then the lamdba function is called again to reduce all the values from each partition to produce one final result.


->On the other hand, when calling groupByKey - all the key-value pairs are shuffled around. 
This is a lot of unnessary data to being transferred over the network.

To determine which machine to shuffle a pair to, Spark calls a partitioning function on the key of the pair. Spark spills data to disk when there is more data shuffled onto a single executor machine than can fit in memory. However, it flushes out the data to disk one key at a time - so if a single key has more key-value pairs than can fit in memory, an out of memory exception occurs. This will be more gracefully handled in a later release of Spark so the job can still proceed, but should still be avoided - when Spark needs to spill to disk, performance is severely impacted.




# sort by vs distribute by vs cluster by vs order by 


Sort by :
                        Hive uses the columns in SORT BY to sort the rows before feeding the rows to a reducer. The sort order will be dependent on the column types. If the column is of numeric type, then the sort order is also in numeric order. If the column is of string type, then the sort order will be lexicographical order.

                        SELECT emp_id, emp_salary FROM employees SORT BY emp_salary DESC;

                        Lets assume the number of reducers were set to 2 and output of each reducer is as follows –



                        Reducer 1 :

                        emp_id | emp_salary
                        10             5000
                        16             3000
                        13             2600
                        19             1800
                        Reducer 2:
                        emp_id | emp_salary
                        11             4000
                        17             3100
                        14             2500
                        20             2000


Order by :

                        ORDER BY
                        This is similar to ORDER BY in SQL Language.

                        In Hive, ORDER BY guarantees total ordering of data, but for that it has to be passed on to a single reducer, which is normally performance intensive and therefore in strict mode, hive makes it compulsory to use LIMIT with ORDER BY so that reducer doesn’t get overburdened.

                        Ordering : Total Ordered data.

                        Outcome : Single output i.e. fully ordered.

                        For example :
                         SELECT emp_id, emp_salary FROM employees ORDER BY emp_salary DESC;


                        Reducer :

                        Shell
                        emp_id | emp_salary
                        10             5000
                        11             4000
                        17             3100
                        16             3000
                        13             2600
                        14             2500
                        20             2000
                        19             1800

DISTRIBUTE BY:
                        Hive uses the columns in Distribute By to distribute the rows among reducers. All rows with the same Distribute By columns will go to the same reducer.

                        
                        It ensures each of N reducers gets non-overlapping ranges of column, but doesn’t sort the output of each reducer. You end up with N or more unsorted files with non-overlapping ranges.

                        Example ( taken directly from Hive wiki ):-

                        We are Distributing By x on the following 5 rows to 2 reducer:

                        x1
                        x2
                        x4
                        x3
                        x1


                        Reducer 1
                        x1
                        x2
                        x1
                        Reducer 2
                        Shell
                        x4
                        x3
                        
CLUSTER BY
            Cluster By is a short-cut for both Distribute By and Sort By.

            CLUSTER BY x ensures each of N reducers gets non-overlapping ranges, then sorts by those ranges at the reducers.

            Ordering : Global ordering between multiple reducers.

            Outcome : N or more sorted files with non-overlapping ranges.

            For the same example as above , if we use Cluster By x, the two reducers will further sort rows on x:

            Reducer 1 :
            x1
            x1
            x2

            Reducer 2 :
            x3
            x4
            1

            Instead of specifying Cluster By, the user can specify Distribute By and Sort By, so the partition columns and sort columns can be different.







# fair and capacity schedulers

Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an equal share of
resources over time. When there is a single job running, that job uses the entire cluster. When other jobs are 
submitted, tasks slots that free up are assigned to the new jobs, so that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. It is also a reasonable way to share a cluster between a number of users. Finally, fair sharing can also work with job priorities - the priorities are used as weights to determine the fraction of total compute time that each job should get.

The CapacityScheduler is designed to allow sharing a large cluster while giving each organization a minimum 
capacity guarantee. The central idea is that the available resources in the Hadoop Map-Reduce cluster are partitioned 
among multiple organizations who collectively fund the cluster based on computing needs. There is an added benefit that
an organization can access any excess capacity no being used by others. This provides elasticity for the organizations 
in a cost-effective manner.

#vcores:

            -
               A VCPU is a core. Your CPU, if Hyperthreaded, doubles your amount of physical cores. 

               Example: You a Quad Core Xeon Processor Socket. It has 4 cores, but it is presented as 8 cores because of hyperthreading.
               
               lscpu
 
Thread(s) per core:    2


            -
                  VCores are virtual machine cores in the Hadoop cluster which is required to process your task (or simply an ability of CPU to compute the job in the cluster). There are some set of configurations which require these virtual cores to be set for all default applications as follows:

                  mapreduce.map.cpu.vcores - The number of virtual cores required for each map task.

                  mapreduce.reduce.cpu.vcores - The number of virtual cores required for each reduce task.

            -
                     “Core” in context of multithreading is a CPU concept - how processes can be run on system in tandem. This can be physical threads of execution added physical power by multiple cores on board and could be coupled with Hyperthreading (Intel)/CoolThreads(Solaris) - with virtual light weight threads on top of physical cores which share hardware resources giving performance boost by better management of logical cores.

                     Abstraction is moved further into application space using Java Multithreading capabilities into Hadoop/YARN in form of vCores for improving resource utilization -

                     Hadoop it can be used to configure threads at Mapper/Reduce job level in mapred-site.xml, where as
                     YARN it is more to manage maximum/minimum threads being used by containers generally by rule executor-cores*num-executors +1 in yarn-site.xml



# Hadoop is OLAP or OLTP?
 Hadoop is OLAP!!!
 
            - Since we use Hadoop to process the analysis of big data & this is done by batch wise on historical data which is loaded in the HDFS( Hadoop distributed file system). Hadoop doesn't provide any random access to the data stored in it's file. So we can't use Hadoop as an OLTP database which is characterized by INSERT -UPDATE- DELETE. hadoop provides access to historical data to carry out an analysis.

            Hence, we can conclude that hadoop is purely an OLAP (online analytical processing).

            The OLAP system does not contain present data but it contains historical or old data. OLAP data is being analyzed in order to generate reports.



             OLAP & OLTP are the techniques of data Warehousing.

            OLTP which is Online transaction processing and

            OLAP which is Online Analytical Processing .

            The difference between both is that OLAP is the reporting engine while OLTP is purely a business process engine.

            To give you a clear idea about both, I would like to give you an example which explains OLAP & OLTP in Layman terms.

            When you go to any big retail store like big Bazaar, Hypercity and after shopping is done you go to payment counter. When the receipt is printed for your name. We think only one transaction happens there that is give and take.

            well , that wasn't the only task happened there. If we had to list down the tasks they would be

            Invoice or receipt is generated.
            Storage of this data in the database.
            Items which you purchased were inserted in the database against invoice number.
            If payement is done by credit card , loyalty points are automatically credited on the card number .
            Stock of those items you purchased automatically reduced from inventory.
            If stock became below desided level , probably auto ordered to the vendor or from the warehouse (most big retail stores has this auto ordering system)
            Order number is generated & tagged against the order number.
            And so many advance task you can add to this list which were triggered at the back end. These tasks are called transactions and the system that is used for processing is called OLTP (online transaction processing)

            Now let's assume there would be 10 transactions for each customer & roughly they get 500 customers daily so total would be 500*10 = 5000 transactions daily for one store. Assuming there are 10 stores , 5000* 10 = 50000 will be the number. At national level this number will go to huge amount. And this is for one day. If we multiply by 30 that would give monthly transactions in millions.

            That's a good number to carry out an analysis & the system which is used to process the analysis is called OLAP (online analytical processing). OLAP is used to analyze the data stored in the data warehouse. This analysis can be useful for sales ,revenue and other operations of the store.

            Since we use Hadoop to process the analysis of big data & this is done by batch wise on historical data which is loaded in the HDFS( Hadoop distributed file system). Hadoop doesn't provide any random access to the data stored in it's file. So we can't use Hadoop as an OLTP database which is characterized by INSERT -UPDATE- DELETE. hadoop provides access to historical data to carry out an analysis.

            Hence, we can conclude that hadoop is purely an OLAP (online analytical processing).

# auxiliary constructor scala 
Scala has two types of constructors:

1. primary constructor

2. auxiliary constructor (secondary constructor)

A scala class can contain zero or more auxiliary constructors. The auxiliary constructor in Scala is used for constructor overloading and defined as a method usingthis  name.
The auxiliary constructor must call either previously defined auxiliary constructors or primary constructors in the first line of its body. Hence, every auxiliary constructor invokes directly or indirectly to a primary constructor.

We can call the primary constructor or other auxiliary constructor using this .

# sharding
What is sharding in nosql, in absolute layman's terms?

A database is like a library: it holds many books. A big library does not fit into a single room. Multiple rooms and buildings are required for big libraries. Now for each couple of rooms, a special librarian person is designated to handle requests regarding those specific 2-3 rooms: requests to get books to read, or to return books. And there are other persons who get all those requests for the whole library, the big one, and they decide, ok, this book has this "hash" (sort of id), that means it must be in that building, and they forward to that building' chief. That guy looks again at the hash and decides, it must be in those rooms, let me ask 2 guys that surely know. So they get to the person actually controlling the room where indeed that book resides. Sharding is just partitioning (splitting) the data set (the set of books) into multiple smaller sets, that can be managed by a single computer.






###############################################################
# Writable and WritableComparable in Hadoop
 kiran  July 28, 2017 0 8,008

This blog helps those people who want to build their own custom types in Hadoop which is possible only with Writable and WritableComparable.

After reading this blog you will get a clear understanding of:

What are Writables?
Importance of Writables in Hadoop
Why are Writables introduced in Hadoop?
What if Writables were not there in Hadoop?
How can Writable and WritableComparable be implemented in Hadoop?
With this knowledge you can get going with Writables and WritableComparables in Hadoop.

Writables and its Importance in Hadoop
Writable is an interface in Hadoop. Writable in Hadoop acts as a wrapper class to almost all the primitive data type of Java. That is how int of java has become IntWritable in Hadoop and String of Java has become Text in Hadoop.

Writables are used for creating serialized data types in Hadoop. So, let us start by understanding what are data type, interface and serilization.

Data Type

A data type is a set of data with values having predefined characteristics. There are several kinds of data types in Java. For example- int, short, byte, long, char etc. These are called as primitive data types. All these primitive data types are bound to classes called as wrapper class. For example int, short, byte, long are grouped under INTEGER which is a wrapper class. These wrapper classes are predefined in the Java.

Interface in Java

An interface in Java is a complete abstract class. The methods within an interface are abstract methods which do not accept body and the fields within the interface are public, static and final, which means that the fields cannot be modified.

The structure of an interface is most likely to be a class. We cannot create an object for an interface and the only way to use the interface is to implement it in other class by using ‘implements’ keyword.

Serialization

Serialization is nothing but converting the raw data into a stream of bytes which can travel along different networks and can reside in different systems. Serialization is not the only concern of Writable interface; it also has to perform compare and sorting operation in Hadoop.

Why are Writables Introduced in Hadoop?
Now the question is whether Writables are necessary for Hadoop. Hadoop framework definitely needs Writable type of interface in order to perform the following tasks:

Implement serialization
Transfer data between clusters and networks
Store the deserialized data in the local disk of the system
Implementation of writable is similar to implementation of interface in Java. It can be done by simply writing the keyword ‘implements’ and overriding the default writable method.

Writable is a strong interface in Hadoop which while serializing the data, reduces the data size enormously, so that data can be exchanged easily within the networks. It has separate read and write fields to read data from network and write data into local disk respectively. Every data inside Hadoop should accept writable and comparable interface properties.

We have seen how Writables reduces the data size overhead and make the data transfer easier in the network.

Hadoop

What if Writable were not there in Hadoop?
Let us now understand what happens if Writable is not present in Hadoop.

Serialization is important in Hadoop because it enables easy transfer of data. If Writable is not present in Hadoop, then it uses the serialization of Java which increases the data over-head in the network.

smallInt serialized value using Java serializer
aced0005737200116a6176612e6c616e672e496e74656765
7212e2a0a4f781873802000149000576616c7565787200106a6176612e
6c616e672e4e756d62657286ac951d0b94e08b020000787000000064
smallInt serialized value using IntWritable
00000064
This shows the clear difference between serialization in Java and Hadoop and also the difference between ObjectInputStream and Writable interface. If the size of serialized data in Hadoop is like that of Java, then it will definitely become an overhead in the network.

Also the core part of Hadoop framework i.e., shuffle and sort phase won’t be executed without using Writable.

How can Writables be Implemneted in Hadoop?
Writable variables in Hadoop have the default properties of Comparable. For example:

When we write a key as IntWritable in the Mapper class and send it to the reducer class, there is an intermediate phase between the Mapper and Reducer class i.e., shuffle and sort, where each key has to be compared with many other keys. If the keys are not comparable, then shuffle and sort phase won’t be executed or may be executed with high amount of overhead.

If a key is taken as IntWritable by default, then it has comparable feature because of RawComparator acting on that variable. It will compare the key taken with the other keys in the network. This cannot take place in the absence of Writable.

Can we make custom Writables? The answer is definitely ‘yes’. We can make our own custom Writable type.

Let us now see how to make a custom type in Java.

The steps to make a custom type in Java is as follows:

public class add {
	int a;
	int b;
	public add() {
		this.a = a;
		this.b = b;
	}
}
Similarly we can make a custom type in Hadoop using Writables.

For implementing Writables, we need few more methods in Hadoop:

public interface Writable {
void readFields(DataInput in);
void write(DataOutput out);
}
Here, readFields, reads the data from network and write will write the data into local disk. Both are necessary for transferring data through clusters. DataInput and DataOutput classes (part of java.io) contain methods to serialize the most basic types of data.

Suppose we want to make a composite key in Hadoop by combining two Writables then follow the steps below:

public class add implements Writable{
public int a;
public int b;
public add(){
this.a=a;
this.b=b;
}
public void write(DataOutput out) throws IOException {
    out.writeInt(a);
    out.writeInt(b);
  }
public void readFields(DataInput in) throws IOException {
    a = in.readInt();
    b = in.readInt();
 }
 public String toString() {
    return Integer.toString(a) + ", " + Integer.toString(b)
 }
}
 

Thus we can create our custom Writables in a way similar to custom types in Java but with two additional methods, write and read Fields. The custom writable can travel through networks and can reside in other systems.

This custom type cannot be compared with each other by default, so again we need to make them comparable with each other.

Let us now discuss what is WritableComparable and the solution to the above problem.

As explained above, if a key is taken as IntWritable, by default it has comparable feature because of RawComparator acting on that variable and it will compare the key taken with the other keys in network and If Writable is not there it won’t be executed.

By default, IntWritable, LongWritable and Text have a RawComparator which can execute this comparable phase for them. Then, will RawComparator help the custom Writable? The answer is no. So, we need to have WritableComparable.

WritableComparable can be defined as a sub interface of Writable, which has the feature of Comparable too. If we have created our custom type writable, then why do we need WritableComparable?

We need to make our custom type, comparable if we want to compare this type with the other.

We want to make our custom type as a key, then we should definitely make our key type as WritableComparable rather than simply Writable. This enables the custom type to be compared with other types and it is also sorted accordingly. Otherwise, the keys won’t be compared with each other and they are just passed through the network.

What happens if WritableComparable is not present?
If we have made our custom type Writable rather than WritableComparable our data won’t be compared with other data types. There is no compulsion that our custom types need to be WritableComparable until unless if it is a key. Because values don’t need to be compared with each other as keys.

If our custom type is a key then we should have WritableComparable or else the data won’t be sorted.

How can WritableComparable be implemented in Hadoop?
The implementation of WritableComparable is similar to Writable but with an additional ‘CompareTo’ method inside it.

public interface WritableComparable extends Writable, Comparable
{
    void readFields(DataInput in);
    void write(DataOutput out);
    int compareTo(WritableComparable o)
}
How to make our custom type, WritableComparable?
We can make custom type a WritableComparable by following the method below:

public class add implements WritableComparable{
public int a;
public int b;
public add(){
this.a=a;
this.b=b;
}
public void write(DataOutput out) throws IOException {
    out.writeint(a);
    out.writeint(b);
  }
public void readFields(DataInput in) throws IOException {
    a = in.readint();
    b = in.readint();
  }
public int CompareTo(add c){
int presentValue=this.value;
int CompareValue=c.value;
return (presentValue < CompareValue ? -1 : (presentValue==CompareValue ? 0 : 1));
}
public int hashCode() {
    return Integer.IntToIntBits(a)^ Integer.IntToIntBits(b);
  }
}
These read fields and write make the comparison of data faster in the network.

With the use of these Writable and WritableComparables in Hadoop, we can make our serialized custom type with less difficulty. This gives the ease for developers to make their custom types based on their requirement.

###############################################################
