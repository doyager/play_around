Index:

Writable and WritableComparable in Hadoop



#hive vs hbase:

hive vs impala:

where should be avro used?
dataframe vs dataset?

hdfs input split vs block size
Hadoop - High avaliability
Secondary vs Backup
Compare Hadoop Files

what happens when you upload a file to hdfs ? walkthrough fsimage edits etc
managed v unmanaged table 
secondary namenode vs stand by namenode 
hive static vs dynamic partioning
hive metastore
managed table vs unmanaged talbe 
how spark supports puthon  , how psakr understand python given it is a jvm kind of Language
apply vs unapply in scala 
kafka offset management ,how does it happen ?
rebalancing kafka 
diff between cache and presist spark 
hadoop journel
hadoop fsimage and edit log 
who manages fsimage
catalyst optimizer - spark ??
advantage of case class in scala 


MR custom partitioner 


kafka - offset managment
kafka - paralleslism vs concurrancy
auxiliary constructor
casaa=ndra nosql
BloomMapFile
pig - physical plan vs logical plan

42.What is BloomMapFile used for? 
The BloomMapFile is a class that extends MapFile. So its functionality is similar to MapFile. BloomMapFile uses dynamic Bloom filters to provide quick membership test for the keys. It is used in Hbase table format.

 When is it a good idea to set Spark locality wait to zero?
https://support.datastax.com/hc/en-us/articles/208237373-FAQ-When-is-it-a-good-idea-to-set-Spark-locality-wait-to-zero-
.setConf("spark.task.maxFailures", "2")
              .setConf("spark.kryoserializer.buffer", "1024")
              .setConf("spark.kryoserializer.buffer.max", "2047")
              .setConf("spark.yarn.maxAppAttempts", "1")
              .setConf("spark.schedular.mode", "FAIR")
              .setConf("spark.locality.wait", "1m")


.setConf("spark.dynamicAllocation.enabled", "true")





#spark 1.6 vs spark 2




# rdd vs dataframe vs dataset 



# hive

#list set commands

#sqoop over write hive 

that --hive-overwrite does not delete existing dir but it overwrites the HDFS data location of hiv

sqoop import --connect jdbc:mysql://localhost/test --username root --password 'hr' --table sample --hive-import --hive-overwrite --hive-table sqoophive -m 1 --fields-terminated-by '\t' --lines-terminated-by '\n'

#RDD Storage levels:

			Storage Level

			Meaning

			MEMORY_ONLY

			Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.

			MEMORY_AND_DISK

			Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.

			MEMORY_ONLY_SER

			Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.

			MEMORY_AND_DISK_SER

			Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.

			DISK_ONLY

			Store the RDD partitions only on disk.

			MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.

			Same as the levels above, but replicate each partition on two cluster nodes.

			OFF_HEAP (experimental)

			Store RDD in serialized format in Tachyon. Compared to MEMORY_ONLY_SER, OFF_HEAP reduces garbage collection overhead and allows executors to be smaller and to share a pool of memory, making it attractive in environments with large heaps or multiple concurrent applications. Furthermore, as the RDDs reside in Tachyon, the crash of an executor does not lead to losing the in-memory cache. In this mode, the memory in Tachyon is discardable. Thus, Tachyon does not attempt to reconstruct a block that it evicts from memory.

####################

#### Hadoop - High avaliability
Secondary vs Backup

What is high availability in Hadoop? Hadoop 2.0 overcomes this SPOF shortcoming by providing support for multiple NameNodes. It introduces Hadoop 2.0 High Availability feature that brings in an extra NameNode (Passive Standby NameNode) to the Hadoop Architecture which is configured for automatic failover

In a typical HA cluster, two separate machines are configured as NameNodes. At any point in time, exactly one of the NameNodes is in an Active state, and the other is in a Standbystate. The Active NameNode is responsible for all client operations in the cluster, while the Standby is simply acting as a slave, maintaining enough state to provide a fast failover if necessary.

In order for the Standby node to keep its state synchronized with the Active node, both nodes communicate with a group of separate daemons called “JournalNodes” (JNs). When any namespace modification is performed by the Active node, it durably logs a record of the modification to a majority of these JNs. The Standby node is capable of reading the edits from the JNs, and is constantly watching them for changes to the edit log. As the Standby Node sees the edits, it applies them to its own namespace. In the event of a failover, the Standby will ensure that it has read all of the edits from the JounalNodes before promoting itself to the Active state. This ensures that the namespace state is fully synchronized before a failover occurs.

In order to provide a fast failover, it is also necessary that the Standby node have up-to-date information regarding the location of blocks in the cluster. In order to achieve this, the DataNodes are configured with the location of both NameNodes, and send block location information and heartbeats to both.

It is vital for the correct operation of an HA cluster that only one of the NameNodes be Active at a time. Otherwise, the namespace state would quickly diverge between the two, risking data loss or other incorrect results. In order to ensure this property and prevent the so-called “split-brain scenario,” the JournalNodes will only ever allow a single NameNode to be a writer at a time. During a failover, the NameNode which is to become active will simply take over the role of writing to the JournalNodes, which will effectively prevent the other NameNode from continuing in the Active state, allowing the new Active to safely proceed with failover.

Reference:
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.ht...

#######################################


### Secondary Name node vs back up name node

But unlike Secondary NameNode or Checkpoint Node, the Backup node does not need to download fsimage and edits files from the active NameNode to create a checkpoint, as it already has an up-to-date state of the namespace in it's own main memory.

Secondary NameNode in hadoop is a specially dedicated node in HDFS cluster whose main function is to take checkpoints of the file system metadata present on namenode. It is not a backup namenode. It just checkpoints namenode’s file system namespace. The Secondary NameNode is a helper to the primary NameNode but not replace for primary namenode. As the NameNode is the single point of failure in HDFS

Ref: http://hadooptutorial.info/tag/secondary-namenode-functions/

#############################

#### Compare Hadoop Files:

Look at the below post which provides an answer on how to compare 2 HDFS files. You will need to extend this for 2 folders.

HDFS File Comparison

You could easily do this with the Java API and create a small app:

FileSystem fs = FileSystem.get(conf);
chksum1 = fs.getFileChecksum(new Path("/path/to/file"));
chksum2 = fs.getFileChecksum(new Path("/path/to/file2"));
return chksum1 == chksum2;


##########################

### Performance Tuning - Spark :

For some workloads, it is possible to improve performance by either caching data in memory, or by turning on some experimental options.

Caching Data In Memory
Spark SQL can cache tables using an in-memory columnar format by calling spark.catalog.cacheTable("tableName") or dataFrame.cache(). Then Spark SQL will scan only required columns and will automatically tune compression to minimize memory usage and GC pressure. You can call spark.catalog.uncacheTable("tableName") to remove the table from memory.

Configuration of in-memory caching can be done using the setConf method on SparkSession or by running SET key=value commands using SQL.

Property Name					Default				Meaning


spark.sql.inMemoryColumnarStorage.compressed	true	When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.

spark.sql.inMemoryColumnarStorage.batchSize	10000	Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.


Other Configuration Options
The following options can also be used to tune the performance of query execution. It is possible that these options will be deprecated in future release as more optimizations are performed automatically.

Property Name			Default				Meaning

spark.sql.files.maxPartitionBytes	134217728 (128 MB)	The maximum number of bytes to pack into a single partition when reading files.


spark.sql.files.openCostInBytes	4194304 (4 MB)	The estimated cost to open a file, measured by the number of bytes could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over-estimated, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).


spark.sql.broadcastTimeout	300	
Timeout in seconds for the broadcast wait time in broadcast joins



spark.sql.autoBroadcastJoinThreshold	10485760 (10 MB)	Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan has been run.



spark.sql.shuffle.partitions	200	Configures the number of partitions to use when shuffling data for joins or aggregations.



Broadcast Hint for SQL Queries


The BROADCAST hint guides Spark to broadcast each specified table when joining them with another table or view. When Spark deciding the join methods, the broadcast hash join (i.e., BHJ) is preferred, even if the statistics is above the configuration spark.sql.autoBroadcastJoinThreshold. When both sides of a join are specified, Spark broadcasts the one having the lower statistics. Note Spark does not guarantee BHJ is always chosen, since not all cases (e.g. full outer join) support BHJ. When the broadcast nested loop join is selected, we still respect the hint.


import org.apache.spark.sql.functions.broadcast
broadcast(spark.table("src")).join(spark.table("records"), "key").show()

############################

# hive static vs dynamic partioning

Partitioning in Hive is very useful to prune data during query to reduce query times.

##static
Partitions are created when data is inserted into table. Depending on how you load data you would need partitions. Usually when loading files (big files) into Hive tables static partitions are preferred. That saves your time in loading data compared to dynamic partition. You "statically" add a partition in table and move the file into the partition of the table. Since the files are big they are usually generated in HDFS. You can get the partition column value form the filename, day of date etc without reading the whole big file.

##dynamic
Incase of dynamic partition whole big file i.e. every row of the data is read and data is partitioned through a MR job into the destination tables depending on certain field in file. So usually dynamic partition are useful when you are doing sort of a ETL flow in your data pipeline. e.g. you load a huge file through a move command into a Table X. then you run a inert query into a Table Y and partition data based on field in table X say day , country. You may want to further run a ETL step to partition the data in country partition in Table Y into a Table Z where data is partitioned based on cities for a particular country only. etc.

Thus depending on your end table or requirements for data and in what form data is produced at source you may choose static or dynamic partition.

#static
in static partitioning we need to specify the partition column value in each and every LOAD statement.

suppose we are having partition on column country for table t1(userid, name,occupation, country), so each time we need to provide country value

hive>LOAD DATA INPATH '/hdfs path of the file' INTO TABLE t1 PARTITION(country="US")
hive>LOAD DATA INPATH '/hdfs path of the file' INTO TABLE t1 PARTITION(country="UK")

#dynamic 
dynamic partition allow us not to specify partition column value each time. the approach we follows is as below:

create a non-partitioned table t2 and insert data into it.
now create a table t1 partitioned on intended column(say country).
load data in t1 from t2 as below:

hive> INSERT INTO TABLE t2 PARTITION(country) SELECT * from T1;
make sure that partitioned column is always the last one in non partitioned table(as we are having country column in t2)


#############################
# hive metastore

What is Hive Metastore?

As we know, the database is among the most important and powerful parts of any organization. It is the collection of Schema, Tables, Relationships, Queries, and Views. It is an organized collection of data.

Metastore is the central repository of Apache Hive metadata. It stores metadata for Hive tables (like their schema and location) and partitions in a relational database. It provides client access to this information by using metastore service API.

All Hive implementations need a metastore service, where it stores metadata. It is implemented using tables in a relational database. By default, Hive uses a built-in Derby SQL server. It provides single process storage, so when we use Derby, we cannot run instances of Hive CLI. Whenever we want to run Hive on a personal machine or for some developer task, then it is good, but when we want to use it in a cluster, then MySQL or any other similar relational database is required.


Databases Supported by Hive

Hive supports 5 backend databases which are as follows:

· Derby

· MySQL

· MS SQL Server

· Oracle

· Postgres



Hive metastore consists of two fundamental units:

1. A service that provides metastore access to other Apache Hive services.

2. Disk storage for the Hive metadata which is separate from HDFS storage.

Hive Metastore Modes

There are three modes for Hive Metastore deployment:

· Embedded Metastore

· Local Metastore

· Remote Metastore

1. Embedded Metastore

In Hive by default, metastore service runs in the same JVM as the Hive service. It uses embedded derby database stored on the local file system in this mode. Thus both metastore service and hive service runs in the same JVM by using embedded Derby Database. But, this mode also has limitation that, as only one embedded Derby database can access the database files on disk at any one time, so only one Hive session could be open at a time.

If we try to start the second session it produces an error when it attempts to open a connection to the metastore. So, to allow many services to connect the Metastore, it configures Derby as a network server. This mode is good for unit testing. But it is not good for the practical solutions.


2 . Local Metastore

Hive is the data-warehousing framework, so hive does not prefer single session. To overcome this limitation of Embedded Metastore, for Local Metastore was introduced. This mode allows us to have many Hive sessions i.e. many users can use the metastore at the same time. We can achieve by using any JDBC compliant like MySQL which runs in a separate JVM or different machines than that of the Hive service and metastore service which are running in the same JVM.

Hive is the data-warehousing framework, so hive does not prefer single session. To overcome this limitation of Embedded Metastore, for Local Metastore was introduced. This mode allows us to have many Hive sessions i.e. many users can use the metastore at the same time. We can achieve by using any JDBC compliant like MySQL which runs in a separate JVM or different machines than that of the Hive service and metastore service which are running in the same JVM.


3. Remote Metastore

Moving further, another metastore configuration called Remote Metastore. In this mode, metastore runs on its own separate JVM, not in the Hive service JVM. If other processes want to communicate with the metastore server they can communicate using Thrift Network APIs. We can also have one more metastore servers in this case to provide more availability. This also brings better manageability/security because the database tier can be completely firewalled off. And the clients no longer need share database credentials with each Hiver user to access the metastore database.

To use this remote metastore, you should configure Hive service by setting hive.metastore.uris to the metastore server URI(s). Metastore server URIs are of the form http://thrift://host:port, where the port corresponds to the one set by METASTORE_PORT when starting the metastore server.



################################

# hdfs input split vs block size

2.1. What is a Block in HDFS?
Hadoop HDFS split large files into small chunks known as Blocks. It contains a minimum amount of data that can be read or write. HDFS stores each file as blocks. The Hadoop application distributes the data block across multiple nodes. HDFS client doesn’t have any control on the block like block location, the Namenode decides all such things. Learn HDFS data Blocks in detail.

2.2. What is InputSplit in Hadoop?
It represents the data which individual mapper processes. Thus the number of map tasks is equal to the number of InputSplits. Framework divides split into records, which mapper processes.

Initially input files store the data for MapReduce job. Input a file typically resides in HDFS InputFormat describes how to split up and read input files. InputFormat is responsible for creating InputSplit. Learn Hadoop InputSplit in detail. 

3. Comparison Between InputSplit vs Blocks in Hadoop
Let’s now discuss the feature wise difference between InputSplit vs Blocks in Hadoop Framework.

3.1. Data Representation
Block – HDFS Block is the physical representation of data in Hadoop.
InputSplit – MapReduce InputSplit is the logical representation of data present in the block in Hadoop. It is basically used during data processing in MapReduce program or other processing techniques. The main thing to focus is that InputSplit doesn’t contain actual data; it is just a reference to the data.
3.2. Size
Block – By default, the HDFS block size is 128MB which you can change as per your requirement. All HDFS blocks are the same size except the last block, which can be either the same size or smaller. Hadoop framework break files into 128 MB blocks and then stores into the Hadoop file system.
InputSplit – InputSplit size by default is approximately equal to block size. It is user defined. In MapReduce program the user can control split size based on the size of data.
3.3. Example of Block and InputSplit in Hadoop
Suppose we need to store the file in HDFS.  Hadoop HDFS stores files as blocks. Block is the smallest unit of data that can be stored or retrieved from the disk. The default size of the block is 128MB. Hadoop HDFS breaks files into blocks. Then it stores these blocks on different nodes in the cluster.

For example, we have a file of 132 MB. So HDFS will break this file into 2 blocks.

Now, if we want to perform a MapReduce operation on the blocks, it will not process. The reason is that 2nd block is incomplete. So, InpuSplit solves this problem. MapReduce InputSplit will form a logical grouping of blocks as a single block. As the InputSplit include a location for the next block and the byte offset of the data needed to complete the block.

4. Conclusion
Hence, InputSplit is only a logical chunk of data i.e. It has just the information about blocks address or location. While Block is the physical representation of data. Now I am sure that, you have a clearer understanding about InputSplit and HDFS Data blocks after reading this blog. If you find any other difference between InputSplit vs Blocks, so do let us know in the comment section.


######################

#hadoop archive
https://community.cloudera.com/t5/Community-Articles/How-HAR-Hadoop-Archive-works/ta-p/249141

How HAR ( Hadoop Archive ) works

Hadoop archives is one of the methodology which is followed to reduce the load on the Namenode by archiving the files and referring all the archives as a single file via har reader.

hadoop archive -archiveName <value> <src> <dsstpath>
Execute hadoop archive commands
sudo -u hdfs hadoop archive -archiveName hartest2.har -p /tmp harSourceFolder2 /tmp/harDestinationFolder2

Hadoop archives are special format archives. A Hadoop archive maps to a file system directory. A Hadoop archive always has a *.har extension. A Hadoop archive directory contains metadata (in the form of _index and _masterindex) and data (part-*) files. The _index file contains the name of the files that are part of the archive and the location within the part files.

How to Create an Archive
Usage: hadoop archive -archiveName name -p <parent> <src>* <dest>



#Hadoop jar commands:

-files <comma separated list of files>	
Specify comma separated files to be copied to the map reduce cluster. Applies only to job.

-libjars <comma seperated list of jars>	Specify
comma separated jar files to include in the classpath. Applies only to job.

-archives <comma separated list of archives>	
Specify comma separated archives to be unarchived on the compute machines. Applies only to job.


# val vs var vs lazy val

“val” is used to define Immutable data. It’s evaluated only once at the time of definition.
“var” is used to define Mutable data. It’s evaluated only once at the time of definition.
Both val and var are evaluated Eagerly.
“lazy val” is used to define Immutable data. It is evaluated only once when we access it for first time. That means it is evaluated Lazily.
“def” is used to define Methods or Functions. It is evaluated only when we access it and evaluated every-time we access it. That means it is evaluated Lazily.




# map vs map partitions vs flat map

Map():

It returns a new RDD by applying the given function to each element of the RDD. 
The function in the map returns only one item.

MapPartitions():

Similar to map, but runs separately on each partition (block) of the RDD, so the function must be of type Iterator<T> ⇒ Iterator<U> when running on an RDD of type T.

flatMap() 
as compared to Map() and MapPartitions(), flatMap() neither works on a single element as map() nor it produces multiple elements of the result as mapPartitions().

Let me explain you with an example. If there are 2000 row and 20 partitions, then each partition will contain the 2000/20=100 Rows.

Now, when we apply map(func) method to rdd, the func() operation will be applied on each and every Row and in this particular case func() operation will be called 2000 times. i.e. time-consuming in some time-critical applications.

If we call mapPartition(func) method on rdd, the func() operation will be called on each partition instead of each row. In this particular case, it will be called 20 times(number of the partition). In this way, you can prevent some processing when it comes to time-consuming application.

Map() exercises function at per element level whereas MapPartitions() exercises function at the partition level.

Now talking about similarity of flatMap() as compared to Map() and MapPartitions(), flatMap() neither works on a single element as map() nor it produces multiple elements of the result as mapPartitions().


# fold vs foldleft vs foldright

Now, the difference between fold, foldLeft, and foldRight.
The primary difference is the order in which the fold operation iterates through the collection in question. foldLeft starts on the left side—the first item—and iterates to the right; foldRight starts on the right side—the last item—and iterates to the left. fold goes in no particular order.

Because fold does not go in any particular order, there are constraints on the start value and thus return value (in all three folds the type of the start value must be the same as the return value).

The first constraint is that the start value must be a supertype of the object you're folding. In our first example we were folding on a type List[Int] and had a start type of Int. Int is a supertype of List[Int].

The second constraint of fold is that the start value must be neutral, i.e. it must not change the result. For example, the neutral value for an addition operation would be 0, 1 for multiplication, Nil lists, etc.



#Spark Garbage Collection Tuning

In garbage collection, tuning in Apache Spark, the first step is to gather statistics on how frequently garbage collection occurs. It also gathers the amount of time spent in garbage collection. Thus, can be achieved by adding -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps to Java option. The next time when Spark job run, a message will display in workers log whenever garbage collection occurs. These logs will be in worker node, not on drivers program.

Java heap space divides into two regions Young and Old. The young generation holds short-lived objects while Old generation holds objects with longer life. The garbage collection tuning aims at, long-lived RDDs in the old generation. It also aims at the size of a young generation which is enough to store short-lived objects. With this, we can avoid full garbage collection to gather temporary object created during task execution. Some steps that may help to achieve this are:

If full garbage collection is invoked several times before a task is complete this ensures that there is not enough memory to execute the task.
In garbage collection statistics, if OldGen is near to full we can reduce the amount of memory used for caching. This can be achieved by lowering spark.memory.fraction. the better choice is to cache fewer objects than to slow down task execution. Or we can decrease the size of young generation i.e., lowering –Xmn.
The effect of Apache Spark garbage collection tuning depends on our application and amount of memory used.


# mapreduce combiner 

                  A Combiner, also known as a semi-reducer, is an optional class that operates by accepting the inputs from the Map class and thereafter passing the output key-value pairs to the Reducer class.

                  The main function of a Combiner is to summarize the map output records with the same key. The output (key-value collection) of the combiner will be sent over the network to the actual Reducer task as input.

                  Combiner
                  The Combiner class is used in between the Map class and the Reduce class to reduce the volume of data transfer between Map and Reduce. Usually, the output of the map task is large and the data transferred to the reduce task is high.

                  Here is a brief summary on how MapReduce Combiner works −

                  A combiner does not have a predefined interface and it must implement the Reducer interface’s reduce() method.

                  A combiner operates on each map output key. It must have the same output key-value types as the Reducer class.

                  A combiner can produce summary information from a large dataset because it replaces the original Map output.

                  Although, Combiner is optional yet it helps segregating data into multiple groups for Reduce phase, which makes it easier to process

# msckrepari - msck repair - metastore check command

          However, users can run a metastore check command with the repair table option:
          -which will update metadata about partitions to the Hive metastore for partitions for which such metadata doesn't already exist. 
          - The default option for MSC command is ADD PARTITIONS. With this option, it will add any partitions that exist on HDFS but not in metastore to the metastore.
          - The DROP PARTITIONS option will remove the partition information from metastore, that is already removed from HDFS. 
          - The SYNC PARTITIONS option is equivalent to calling both ADD and DROP PARTITIONS

           When there is a large number of untracked partitions, there is a provision to run MSCK REPAIR TABLE batch wise to avoid OOME (Out of Memory Error). By giving the configured batch size for the property hive.msck.repair.batch.size it can run in the batches internally. The default value of the property is zero, it means it will execute all the partitions at once. MSCK command without the REPAIR option can be used to find details about metadata mismatch metastore.

          The equivalent command on Amazon Elastic MapReduce (EMR)'s version of Hive is:

          ALTER TABLE table_name RECOVER PARTITIONS;

          MSCK [REPAIR] TABLE table_name [ADD/DROP/SYNC PARTITIONS];

          Recover Partitions (MSCK REPAIR TABLE)
          Hive stores a list of partitions for each table in its metastore. If, however, new partitions are directly added to HDFS (say by using hadoop fs -put command) or removed from HDFS, the metastore (and hence Hive) will not be aware of these changes to partition information unless the user runs ALTER TABLE table_name ADD/DROP PARTITION commands on each of the newly added or removed partitions, respectively.




# groubykey vs reduce by key 

https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html

    ->Avoid GroupByKey
        Let's look at two different ways to compute word counts, one using reduceByKey and the other using groupByKey:

        val words = Array("one", "two", "two", "three", "three", "three")
        val wordPairsRDD = sc.parallelize(words).map(word => (word, 1))

        Code:
                        val wordCountsWithReduce = wordPairsRDD
                          .reduceByKey(_ + _)
                          .collect()

                        val wordCountsWithGroup = wordPairsRDD
                          .groupByKey()
                          .map(t => (t._1, t._2.sum))
                          .collect()
                  
                  
->While both of these functions will produce the correct answer, 
->the reduceByKey example works much better on a large dataset. 
That's because Spark knows it can combine output with a common key on each partition before shuffling the data.

Look at the diagram below to understand what happens with reduceByKey. Notice how pairs on the same machine with the same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled. Then the lamdba function is called again to reduce all the values from each partition to produce one final result.


->On the other hand, when calling groupByKey - all the key-value pairs are shuffled around. 
This is a lot of unnessary data to being transferred over the network.

To determine which machine to shuffle a pair to, Spark calls a partitioning function on the key of the pair. Spark spills data to disk when there is more data shuffled onto a single executor machine than can fit in memory. However, it flushes out the data to disk one key at a time - so if a single key has more key-value pairs than can fit in memory, an out of memory exception occurs. This will be more gracefully handled in a later release of Spark so the job can still proceed, but should still be avoided - when Spark needs to spill to disk, performance is severely impacted.




# sort by vs distribute by vs cluster by vs order by 


Sort by :
                        Hive uses the columns in SORT BY to sort the rows before feeding the rows to a reducer. The sort order will be dependent on the column types. If the column is of numeric type, then the sort order is also in numeric order. If the column is of string type, then the sort order will be lexicographical order.

                        SELECT emp_id, emp_salary FROM employees SORT BY emp_salary DESC;

                        Lets assume the number of reducers were set to 2 and output of each reducer is as follows –



                        Reducer 1 :

                        emp_id | emp_salary
                        10             5000
                        16             3000
                        13             2600
                        19             1800
                        Reducer 2:
                        emp_id | emp_salary
                        11             4000
                        17             3100
                        14             2500
                        20             2000


Order by :

                        ORDER BY
                        This is similar to ORDER BY in SQL Language.

                        In Hive, ORDER BY guarantees total ordering of data, but for that it has to be passed on to a single reducer, which is normally performance intensive and therefore in strict mode, hive makes it compulsory to use LIMIT with ORDER BY so that reducer doesn’t get overburdened.

                        Ordering : Total Ordered data.

                        Outcome : Single output i.e. fully ordered.

                        For example :
                         SELECT emp_id, emp_salary FROM employees ORDER BY emp_salary DESC;


                        Reducer :

                        Shell
                        emp_id | emp_salary
                        10             5000
                        11             4000
                        17             3100
                        16             3000
                        13             2600
                        14             2500
                        20             2000
                        19             1800

DISTRIBUTE BY:
                        Hive uses the columns in Distribute By to distribute the rows among reducers. All rows with the same Distribute By columns will go to the same reducer.

                        
                        It ensures each of N reducers gets non-overlapping ranges of column, but doesn’t sort the output of each reducer. You end up with N or more unsorted files with non-overlapping ranges.

                        Example ( taken directly from Hive wiki ):-

                        We are Distributing By x on the following 5 rows to 2 reducer:

                        x1
                        x2
                        x4
                        x3
                        x1


                        Reducer 1
                        x1
                        x2
                        x1
                        Reducer 2
                        Shell
                        x4
                        x3
                        
CLUSTER BY
            Cluster By is a short-cut for both Distribute By and Sort By.

            CLUSTER BY x ensures each of N reducers gets non-overlapping ranges, then sorts by those ranges at the reducers.

            Ordering : Global ordering between multiple reducers.

            Outcome : N or more sorted files with non-overlapping ranges.

            For the same example as above , if we use Cluster By x, the two reducers will further sort rows on x:

            Reducer 1 :
            x1
            x1
            x2

            Reducer 2 :
            x3
            x4
            1

            Instead of specifying Cluster By, the user can specify Distribute By and Sort By, so the partition columns and sort columns can be different.







# fair and capacity schedulers

Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an equal share of
resources over time. When there is a single job running, that job uses the entire cluster. When other jobs are 
submitted, tasks slots that free up are assigned to the new jobs, so that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. It is also a reasonable way to share a cluster between a number of users. Finally, fair sharing can also work with job priorities - the priorities are used as weights to determine the fraction of total compute time that each job should get.

The CapacityScheduler is designed to allow sharing a large cluster while giving each organization a minimum 
capacity guarantee. The central idea is that the available resources in the Hadoop Map-Reduce cluster are partitioned 
among multiple organizations who collectively fund the cluster based on computing needs. There is an added benefit that
an organization can access any excess capacity no being used by others. This provides elasticity for the organizations 
in a cost-effective manner.

#vcores:

            -
               A VCPU is a core. Your CPU, if Hyperthreaded, doubles your amount of physical cores. 

               Example: You a Quad Core Xeon Processor Socket. It has 4 cores, but it is presented as 8 cores because of hyperthreading.
               
               lscpu
 
Thread(s) per core:    2


            -
                  VCores are virtual machine cores in the Hadoop cluster which is required to process your task (or simply an ability of CPU to compute the job in the cluster). There are some set of configurations which require these virtual cores to be set for all default applications as follows:

                  mapreduce.map.cpu.vcores - The number of virtual cores required for each map task.

                  mapreduce.reduce.cpu.vcores - The number of virtual cores required for each reduce task.

            -
                     “Core” in context of multithreading is a CPU concept - how processes can be run on system in tandem. This can be physical threads of execution added physical power by multiple cores on board and could be coupled with Hyperthreading (Intel)/CoolThreads(Solaris) - with virtual light weight threads on top of physical cores which share hardware resources giving performance boost by better management of logical cores.

                     Abstraction is moved further into application space using Java Multithreading capabilities into Hadoop/YARN in form of vCores for improving resource utilization -

                     Hadoop it can be used to configure threads at Mapper/Reduce job level in mapred-site.xml, where as
                     YARN it is more to manage maximum/minimum threads being used by containers generally by rule executor-cores*num-executors +1 in yarn-site.xml



# Hadoop is OLAP or OLTP?
 Hadoop is OLAP!!!
 
            - Since we use Hadoop to process the analysis of big data & this is done by batch wise on historical data which is loaded in the HDFS( Hadoop distributed file system). Hadoop doesn't provide any random access to the data stored in it's file. So we can't use Hadoop as an OLTP database which is characterized by INSERT -UPDATE- DELETE. hadoop provides access to historical data to carry out an analysis.

            Hence, we can conclude that hadoop is purely an OLAP (online analytical processing).

            The OLAP system does not contain present data but it contains historical or old data. OLAP data is being analyzed in order to generate reports.



             OLAP & OLTP are the techniques of data Warehousing.

            OLTP which is Online transaction processing and

            OLAP which is Online Analytical Processing .

            The difference between both is that OLAP is the reporting engine while OLTP is purely a business process engine.

            To give you a clear idea about both, I would like to give you an example which explains OLAP & OLTP in Layman terms.

            When you go to any big retail store like big Bazaar, Hypercity and after shopping is done you go to payment counter. When the receipt is printed for your name. We think only one transaction happens there that is give and take.

            well , that wasn't the only task happened there. If we had to list down the tasks they would be

            Invoice or receipt is generated.
            Storage of this data in the database.
            Items which you purchased were inserted in the database against invoice number.
            If payement is done by credit card , loyalty points are automatically credited on the card number .
            Stock of those items you purchased automatically reduced from inventory.
            If stock became below desided level , probably auto ordered to the vendor or from the warehouse (most big retail stores has this auto ordering system)
            Order number is generated & tagged against the order number.
            And so many advance task you can add to this list which were triggered at the back end. These tasks are called transactions and the system that is used for processing is called OLTP (online transaction processing)

            Now let's assume there would be 10 transactions for each customer & roughly they get 500 customers daily so total would be 500*10 = 5000 transactions daily for one store. Assuming there are 10 stores , 5000* 10 = 50000 will be the number. At national level this number will go to huge amount. And this is for one day. If we multiply by 30 that would give monthly transactions in millions.

            That's a good number to carry out an analysis & the system which is used to process the analysis is called OLAP (online analytical processing). OLAP is used to analyze the data stored in the data warehouse. This analysis can be useful for sales ,revenue and other operations of the store.

            Since we use Hadoop to process the analysis of big data & this is done by batch wise on historical data which is loaded in the HDFS( Hadoop distributed file system). Hadoop doesn't provide any random access to the data stored in it's file. So we can't use Hadoop as an OLTP database which is characterized by INSERT -UPDATE- DELETE. hadoop provides access to historical data to carry out an analysis.

            Hence, we can conclude that hadoop is purely an OLAP (online analytical processing).

# auxiliary constructor scala 
Scala has two types of constructors:

1. primary constructor

2. auxiliary constructor (secondary constructor)

A scala class can contain zero or more auxiliary constructors. The auxiliary constructor in Scala is used for constructor overloading and defined as a method usingthis  name.
The auxiliary constructor must call either previously defined auxiliary constructors or primary constructors in the first line of its body. Hence, every auxiliary constructor invokes directly or indirectly to a primary constructor.

We can call the primary constructor or other auxiliary constructor using this .

# sharding
What is sharding in nosql, in absolute layman's terms?

A database is like a library: it holds many books. A big library does not fit into a single room. Multiple rooms and buildings are required for big libraries. Now for each couple of rooms, a special librarian person is designated to handle requests regarding those specific 2-3 rooms: requests to get books to read, or to return books. And there are other persons who get all those requests for the whole library, the big one, and they decide, ok, this book has this "hash" (sort of id), that means it must be in that building, and they forward to that building' chief. That guy looks again at the hash and decides, it must be in those rooms, let me ask 2 guys that surely know. So they get to the person actually controlling the room where indeed that book resides. Sharding is just partitioning (splitting) the data set (the set of books) into multiple smaller sets, that can be managed by a single computer.



###########################################
# map reduce custom partitioner MR
# https://acadgild.com/blog/mapreduce-custom-partitioner

What is Custom Partitioner?

Custom Partitioner is a process that allows you to store the results in different reducers, based on the user condition. By setting a partitioner to partition by the key, we can guarantee that, records for the same key will go to the same reducer. A partitioner ensures that only one reducer receives all the records for that particular key.

How Partitioning is done in Hadoop?

HashPartitioner is the default partitioner in Hadoop, which creates one Reduce task for each unique “key”.  All the values with the same key goes to the same instance of your reducer, in a single call to the reduce function.

If user is interested to store a particular group of results in different reducers, then the user can write his own partitioner implementation. It can be general purpose or custom made to the specific data types or values that you expect to use in user application.


mport java.io.IOException;

import org.apache.hadoop.conf.Configuration;

import org.apache.hadoop.conf.Configured;

import org.apache.hadoop.fs.Path;

import org.apache.hadoop.io.LongWritable;

import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Job;

import org.apache.hadoop.mapreduce.Mapper;

import org.apache.hadoop.mapreduce.Partitioner;

import org.apache.hadoop.mapreduce.Reducer;

import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;

import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import org.apache.hadoop.util.Tool;

import org.apache.hadoop.util.ToolRunner;

MapPartitioner:

1. public class Sp extends Configured implements Tool

2. {

3. public static class MapPartitioner extends Mapper<LongWritable, Text, Text, Text>

4. {

5. public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException

6. {

7. String[] tokens = value.toString().split(“,”);

8. String emp_dept = tokens[2].toString();

9. String emp_id_n_ds_sal = tokens[0]+”,”+tokens[1]+”,”+tokens[3]+”,”+tokens[4]+”,”+tokens[5];

10. context.write(new Text(emp_dept), new Text(emp_id_n_ds_sal));

11. }

12. }

The following is the explanation of the above mapper code:

In Line 1, we are taking class by name Sp, which extends Configured class properties and implements Tool interface.
In Line 3, the mapper default class is extended to include the arguments KeyIn as LongWritable, ValueIn as Text, KeyOut as Text and ValueOut as Text.
In Line 5, we are overriding the map method, which will run one time for every line.
In Line 7, we are converting our input value to string and splitting the line by using comma “,” delimiter and storing the values in a String Array, so that all the columns in a row are stored in the string array.
In Line 8, we are storing the employee department name column, which is in the 3rd column in our input file Emp.
In Line 9, we are storing rest of the employee information, i.e., employee id, employee name, employee designation and employee salary in a String variable emp_id_n_ds_sal.
Hadoop

DeptPartitioner:

13. public static class DeptPartitioner extends Partitioner<Text, Text>

14. {

15. @Override

16. public int getPartition(Text key, Text value, int numReduceTasks)

17. {

18. String emp_dept = key.toString();

19. if(numReduceTasks == 0)

20. return 0;

21. if(key.equals(new Text(“Program Department”)))

22. {

23. return 0;

24. }

25. else if(key.equals(new Text(“Admin Department”)))

26. {

27. return 1 % numReduceTasks;

28. }

29. else

30. return 2 % numReduceTasks;

31. }

32. }

The following is the explanation of the above mapper code:

In Line 13, we are taking class by name DeptPartitioner, which extends Partitioner class.
In Line 16, the command overrides the getPartition function, which has three parameters. The key and value are the intermediate key and value produced by the map function. The numReduceTasks is the number of reducers used in the MapReduce program and is specified in the driver program.
In Line 17, we are converting key department name from Text type to String data type and storing the key value department name in the String variable emp_dept.
In Line 18, we are including a condition, where if the number of reduce task is zero then we exit (Line 20) or else proceed (Line 21).
In Line 21, we are including a condition, where if the key value name equals to department name “Program Department”, then we will return to 1st reducer.
In Line 25, we are including a condition else, where if the key value name equals to department name “Admin Department”, then we will return to 2nd reducer.
In Line 29, we are including a condition else, where if the key value name is not equal to the department name “Program Department” and “Admin Department”, then we must return to the 3rd reducer.
ReducePartition:

33. static class ReduceParitioner extends Reducer<Text, Text, Text, Text>

34. {

35. @Override

36. public void reduce(Text key, Iterable<Text> values, Context context)throws IOException, InterruptedException

37. {

38. int max_sal = Integer.MIN_VALUE;

39. String emp_name = ” “;

40. String emp_dept = ” “;

41. String emp_des = ” “;

42. String emp_id = ” “;

43. int emp_sal = 0;

44. for(Text val: values)

45. {

46. String [] valTokens = val.toString().split(“,”);

47. emp_sal = Integer.parseInt(valTokens[3]);

48. if(emp_sal > max_sal)

49. {

50. emp_id = valTokens[0];

51. emp_name = valTokens[1];

52. emp_des = valTokens[2];

53. emp_dept =key.toString();

54. max_sal = emp_sal;

55. }

56. }

57. context.write(new Text(emp_dept), new Text(“id=>”+emp_id+”,name=>”+emp_name+”,des=>”+emp_des+”,sal=>”+max_sal));

58. }

59. }

Line 33 extends the default Reducer class with arguments KeyIn as Text and ValueIn as Text, which are same as the outputs of the mapper class and KeyOut as Text and ValueOut as Text which will be the final outputs of our MapReduce program.
In Line 35, we are overriding the Reduce method, which will run each time for every key.
In Line 38, we are declaring an integer variable max_sal, which will store the highest employee salary.
In Line 39, we are declaring a String variable emp_name, which will store the employee name.
In Line 40, we are declaring a String variable emp_dept, which will store the employee department name.
In Line 41, we are declaring a String variable emp_des, which will store the employee designation.
In Line 42, we are declaring a String variable emp_id, which will store the employee id.
In Line 43, we are declaring an integer variable emp_sal and initialized it to zero.
In Line 44, a foreach loop is taken and will run each time for the values inside the “Iterable values” coming from the shuffle and sort phase after the mapper phase.
In Line 46, we are converting the input value line to string and splitting the line columns by using comma “,” delimiter and storing the values in a String Array so that all the columns in a row are stored in the string array.
In Line 47, we are converting the column 4 of the String array valTokens into integer type and storing the each value into the emp_sal variable.
In line 48, we are including a condition, where if the employee salary is greater than max salary, then we proceed.
In line 50, we are storing String valTokens 0th column employee id value into emp_id variable.
In line 51, we are storing String valTokens 1st column employee name value into emp_name variable.
In line 52, we are storing String valTokens 2nd column employee designation name value into emp_des variable.
In line 53, we are converting the partitioned key value into String type and storing the key value into the variable emp_dept.
In line 54, we are storing highest paid employee salary in a particular department name into the variable max_sal.
Line 57 writes the respected key (department name) and the obtained rest of the employee details (employee id, employee name, employee designation name and employee salary) as value to the context.
Conf Code:

We are partitioning keys based on the “Department Name” category, and we have 3 different department names which are available in the dataset and hence we have partitioned number of reduce tasks to 3. Below are the details of the Driver method:

60. public int run(String[] arg) throws Exception

61. {

62. Configuration conf = getConf();

63. Job job = new Job(conf, “Maxsal”);

64. job.setJarByClass(Sp.class);

65. FileInputFormat.setInputPaths(job, new Path(arg[0]));

66. FileOutputFormat.setOutputPath(job,new Path(arg[1]));

67. job.setMapperClass(MapPartitioner.class);

68. job.setMapOutputKeyClass(Text.class);

69. job.setMapOutputValueClass(Text.class);

70. job.setPartitionerClass(DeptPartitioner.class);

71. job.setReducerClass(ReduceParitioner.class);

72. job.setNumReduceTasks(3);

73. job.setInputFormatClass(TextInputFormat.class);

74. job.setOutputFormatClass(TextOutputFormat.class);

75. job.setOutputKeyClass(Text.class);

76. job.setOutputValueClass(Text.class);

77. System.exit(job.waitForCompletion(true)? 0 : 1);

78. return 0;

79. }

80. public static void main(String[] args) throws Exception

81. {

82. int res = ToolRunner.run(new Configuration(), new Sp(), args);

83. if (args.length != 2)

84. {

85. System.err.println(“Usage: SamplePartitioner <in> <output>”);

86. System.exit(2);

87. }

88. System.exit(res);

89. }

90. }

###############################################################
# Writable and WritableComparable in Hadoop
 kiran  July 28, 2017 0 8,008

This blog helps those people who want to build their own custom types in Hadoop which is possible only with Writable and WritableComparable.

After reading this blog you will get a clear understanding of:

What are Writables?
Importance of Writables in Hadoop
Why are Writables introduced in Hadoop?
What if Writables were not there in Hadoop?
How can Writable and WritableComparable be implemented in Hadoop?
With this knowledge you can get going with Writables and WritableComparables in Hadoop.

Writables and its Importance in Hadoop
Writable is an interface in Hadoop. Writable in Hadoop acts as a wrapper class to almost all the primitive data type of Java. That is how int of java has become IntWritable in Hadoop and String of Java has become Text in Hadoop.

Writables are used for creating serialized data types in Hadoop. So, let us start by understanding what are data type, interface and serilization.

Data Type

A data type is a set of data with values having predefined characteristics. There are several kinds of data types in Java. For example- int, short, byte, long, char etc. These are called as primitive data types. All these primitive data types are bound to classes called as wrapper class. For example int, short, byte, long are grouped under INTEGER which is a wrapper class. These wrapper classes are predefined in the Java.

Interface in Java

An interface in Java is a complete abstract class. The methods within an interface are abstract methods which do not accept body and the fields within the interface are public, static and final, which means that the fields cannot be modified.

The structure of an interface is most likely to be a class. We cannot create an object for an interface and the only way to use the interface is to implement it in other class by using ‘implements’ keyword.

Serialization

Serialization is nothing but converting the raw data into a stream of bytes which can travel along different networks and can reside in different systems. Serialization is not the only concern of Writable interface; it also has to perform compare and sorting operation in Hadoop.

Why are Writables Introduced in Hadoop?
Now the question is whether Writables are necessary for Hadoop. Hadoop framework definitely needs Writable type of interface in order to perform the following tasks:

Implement serialization
Transfer data between clusters and networks
Store the deserialized data in the local disk of the system
Implementation of writable is similar to implementation of interface in Java. It can be done by simply writing the keyword ‘implements’ and overriding the default writable method.

Writable is a strong interface in Hadoop which while serializing the data, reduces the data size enormously, so that data can be exchanged easily within the networks. It has separate read and write fields to read data from network and write data into local disk respectively. Every data inside Hadoop should accept writable and comparable interface properties.

We have seen how Writables reduces the data size overhead and make the data transfer easier in the network.

Hadoop

What if Writable were not there in Hadoop?
Let us now understand what happens if Writable is not present in Hadoop.

Serialization is important in Hadoop because it enables easy transfer of data. If Writable is not present in Hadoop, then it uses the serialization of Java which increases the data over-head in the network.

smallInt serialized value using Java serializer
aced0005737200116a6176612e6c616e672e496e74656765
7212e2a0a4f781873802000149000576616c7565787200106a6176612e
6c616e672e4e756d62657286ac951d0b94e08b020000787000000064
smallInt serialized value using IntWritable
00000064
This shows the clear difference between serialization in Java and Hadoop and also the difference between ObjectInputStream and Writable interface. If the size of serialized data in Hadoop is like that of Java, then it will definitely become an overhead in the network.

Also the core part of Hadoop framework i.e., shuffle and sort phase won’t be executed without using Writable.

How can Writables be Implemneted in Hadoop?
Writable variables in Hadoop have the default properties of Comparable. For example:

When we write a key as IntWritable in the Mapper class and send it to the reducer class, there is an intermediate phase between the Mapper and Reducer class i.e., shuffle and sort, where each key has to be compared with many other keys. If the keys are not comparable, then shuffle and sort phase won’t be executed or may be executed with high amount of overhead.

If a key is taken as IntWritable by default, then it has comparable feature because of RawComparator acting on that variable. It will compare the key taken with the other keys in the network. This cannot take place in the absence of Writable.

Can we make custom Writables? The answer is definitely ‘yes’. We can make our own custom Writable type.

Let us now see how to make a custom type in Java.

The steps to make a custom type in Java is as follows:

public class add {
	int a;
	int b;
	public add() {
		this.a = a;
		this.b = b;
	}
}
Similarly we can make a custom type in Hadoop using Writables.

For implementing Writables, we need few more methods in Hadoop:

public interface Writable {
void readFields(DataInput in);
void write(DataOutput out);
}
Here, readFields, reads the data from network and write will write the data into local disk. Both are necessary for transferring data through clusters. DataInput and DataOutput classes (part of java.io) contain methods to serialize the most basic types of data.

Suppose we want to make a composite key in Hadoop by combining two Writables then follow the steps below:

public class add implements Writable{
public int a;
public int b;
public add(){
this.a=a;
this.b=b;
}
public void write(DataOutput out) throws IOException {
    out.writeInt(a);
    out.writeInt(b);
  }
public void readFields(DataInput in) throws IOException {
    a = in.readInt();
    b = in.readInt();
 }
 public String toString() {
    return Integer.toString(a) + ", " + Integer.toString(b)
 }
}
 

Thus we can create our custom Writables in a way similar to custom types in Java but with two additional methods, write and read Fields. The custom writable can travel through networks and can reside in other systems.

This custom type cannot be compared with each other by default, so again we need to make them comparable with each other.

Let us now discuss what is WritableComparable and the solution to the above problem.

As explained above, if a key is taken as IntWritable, by default it has comparable feature because of RawComparator acting on that variable and it will compare the key taken with the other keys in network and If Writable is not there it won’t be executed.

By default, IntWritable, LongWritable and Text have a RawComparator which can execute this comparable phase for them. Then, will RawComparator help the custom Writable? The answer is no. So, we need to have WritableComparable.

WritableComparable can be defined as a sub interface of Writable, which has the feature of Comparable too. If we have created our custom type writable, then why do we need WritableComparable?

We need to make our custom type, comparable if we want to compare this type with the other.

We want to make our custom type as a key, then we should definitely make our key type as WritableComparable rather than simply Writable. This enables the custom type to be compared with other types and it is also sorted accordingly. Otherwise, the keys won’t be compared with each other and they are just passed through the network.

What happens if WritableComparable is not present?
If we have made our custom type Writable rather than WritableComparable our data won’t be compared with other data types. There is no compulsion that our custom types need to be WritableComparable until unless if it is a key. Because values don’t need to be compared with each other as keys.

If our custom type is a key then we should have WritableComparable or else the data won’t be sorted.

How can WritableComparable be implemented in Hadoop?
The implementation of WritableComparable is similar to Writable but with an additional ‘CompareTo’ method inside it.

public interface WritableComparable extends Writable, Comparable
{
    void readFields(DataInput in);
    void write(DataOutput out);
    int compareTo(WritableComparable o)
}
How to make our custom type, WritableComparable?
We can make custom type a WritableComparable by following the method below:

public class add implements WritableComparable{
public int a;
public int b;
public add(){
this.a=a;
this.b=b;
}
public void write(DataOutput out) throws IOException {
    out.writeint(a);
    out.writeint(b);
  }
public void readFields(DataInput in) throws IOException {
    a = in.readint();
    b = in.readint();
  }
public int CompareTo(add c){
int presentValue=this.value;
int CompareValue=c.value;
return (presentValue < CompareValue ? -1 : (presentValue==CompareValue ? 0 : 1));
}
public int hashCode() {
    return Integer.IntToIntBits(a)^ Integer.IntToIntBits(b);
  }
}
These read fields and write make the comparison of data faster in the network.

With the use of these Writable and WritableComparables in Hadoop, we can make our serialized custom type with less difficulty. This gives the ease for developers to make their custom types based on their requirement.

############################
MPP:

https://www.dezyre.com/article/impala-vs-hive-difference-between-sql-on-hadoop-components/180


###############################################################

hive vs impala:


Key Difference Between Hive and Impala
- Hive is written in Java but Impala is written in C++. 
- Query processing speed in Hive is slow but Impala is 6-69 times faster than Hive.
- Hive supports complex type but Impala does not support complex types.
-Hive is batch-based Hadoop MapReduce but Impala is MPP database.

What is the advantage of using Impala over hive?
Cloudera Impala was developed to resolve the limitations posed by low interaction of Hadoop Sql.
Cloudera Impala provides low latency high performance SQL like queries to process and analyze data with only one
condition that the data be stored on Hadoop clusters

Why is Impala faster than Hive?
- Why Impala is faster than Hive in query processing. ... While processing SQL-like queries, Impala does not write intermediate results on disk; instead full SQL processing is done in memory, which makes it faster.
- While processing SQL-like queries, Impala does not write intermediate results on disk; instead full SQL processing is done in memory, which makes it faster.
- With Impala, the query starts its execution instantly compared to MapReduce, which may take significant time to start processing larger SQL queries and this adds more time in processing.
- Impala Query Planner uses smart algorithms to execute queries in multiple stages ..

################################
impala:

Impala - Architecture. Impala is an MPP (Massive Parallel Processing) query execution engine that runs on a number of systems in the Hadoop cluster.It has three main components namely, Impala daemon (Impalad), Impala Statestore, and Impala metadata or metastore.

https://www.dezyre.com/article/impala-vs-hive-difference-between-sql-on-hadoop-components/180
Impala massively improves on the performance parameters as it eliminates the need to migrate huge data sets to dedicated processing systems or convert data formats prior to analysis. Salient features of Impala include:

Hadoop Distributed File System (HDFS) and Apache HBase storage support
Recognizes Hadoop file formats, text, LZO, SequenceFile, Avro, RCFile and Parquet
Supports Hadoop Security (Kerberos authentication)
Fine – grained, role-based authorization with Apache Sentry
Can easily read metadata, ODBC driver and SQL syntax from Apache Hive
Impala’s rise within a short span of little over 2 years can be gauged from the fact that Amazon Web Services and MapR have both added support for it.

https://www.dezyre.com/article/impala-vs-hive-difference-between-sql-on-hadoop-components/180
- What is Impala? :::::::
Step aside, the SQL engines claiming to do parallel processing! Impala’s open source Massively Parallel Processing (MPP) SQL engine is here, armed with all the power to push you aside. The only condition it needs is data be stored in a cluster of computers running Apache Hadoop, which, given Hadoop’s dominance in data warehousing, isn’t uncommon. Cloudera Impala was announced on the world stage in October 2012 and after a successful beta run, was made available to the general public in May 2013.


Cloudera Impala is an excellent choice for programmers for running queries on HDFS and Apache HBase as it doesn’t require data to be moved or transformed prior to processing. Cloudera Impala easily integrates with Hadoop ecosystem, as its file and data formats, metadata, security and resource management frameworks are same as those used by MapReduce, Apache Hive, Apache Pig and other Hadoop software. It is architected specifically to assimilate the strengths of Hadoop and the familiarity of SQL support and multi user performance of traditional database. Its unified resource management across frameworks has made it the de facto standard for open source interactive business intelligence tasks.

Cloudera Impala has the following two technologies that give other processing languages a run for their money:

	1. Columnar Storage::::::::::
	Data is stored in columnar fashion which achieves high compression ratio and efficient scanning.
	2. Tree Architecture::::::::
	This is fundamental to attaining a massively parallel distributed multi – level serving tree for pushing down a query to the tree and then aggregating the results from the leaves


- ans 2 :
The architecture is similar to the other distributed databases like Netezza, Greenplum etc. Hadoop impala consists of different daemon processes that run on specific hosts within your CDH cluster.

- Cloudera Hadoop Impala Architecture Overview::::::::::
https://dwgeek.com/introduction-hadoop-impala-architecture.html/

SQL Interface ->Goes to Both [1. HDFSNN, Hivemetastore,StateStore] , [2. then to Data node Imapala demon , where it does query planning coordination , and executions on local HDFS DN and others]

- On each Data node 
	[<Query Planner > -> <Query Coordinator> -> <Query Exec Engine> -> HDFS DN]
	
The Hadoop impala is consists of three components: The Impala Daemon, Impala Statestore and Impala Catalog Services:

- The Impala Daemon :
	Impala Daemon is the important and core component of the Hadoop Impala. 
		-> This daemon runs on every node in the CDH cluster. 
	This is identified by the impalad process. 
	It reads and writes the data files. It also accepts the queries transmitted from impala-shell command, ODBC, JDBC or Hue.

	You can connect and submit the query to the Impala Daemon running on any Datanode and that instance of Daemon serves as coordinator. The Daemon which accepts the queries acts as a coordinator, that parallizes the queries and distributes the workload across the Hadoop cluster. It also collects the results back from all nodes.

	Impala Daemon will always be communicating to statestore to confirm which node is healthy and accepts the new work. Each Daemon will also receive the broadcasted message whenever any Impala node in cluster create, alter, drops any object or any statement like insert, load data is processed.


-The Impala Statestore : -

	This component checks health of all Impala Daemons on all the datanodes in the Hadoop cluster. It is physically represented by the process statestored. 
			-> Only one such process is required on one host in the Hadoop cluster.
			-> If an Impala Daemon goes down, statestore informs all the Impala Daemons so that they can avoid the failed node while distributing future queries.

- The impala Catalog Service:-
		This component of the Hadoop Impala tells metadata changes from Impala SQL statements to all the Datanodes in Hadoop cluster. It is physically rhaepresented by Daemon process catalogd.
		 -> Only one such process is required on one host in the Hadoop cluster. U
		 -> Usually, statestored and catalogd process will be running on same host as catalog services are passed through statestored.

		The catalog service avoids the need to issue REFRESH and INVALIDATE METADATA statements when the metadata changes are performed by statements issued through Impala.

Why is it used? 
Impala gives parallel processing database technology on top of Hadoop eco-system. It allows users to perform low latency queries interactively. Hive MapReduce job will take some minimum time in launching and processing queries where as impala gives results in seconds.

Does Impala use MapReduce?
Impala does not make use of Mapreduce as it contains its own pre-defined daemon process to run a job. It sits on top of only the Hadoop Distributed File System (HDFS) as it uses the same to merely store the data. Therefore, we prefer calling it as simply “SQL on HDFS”

Why is impala not Tolerant?
If you run a query on hive there is starttime overhead on queries run on mapreduce but not on impala. Hive is fault tolerant where as impala is not. For e.g. if you run a query in hive mapreduce and while the query is running one of your datanode goes down still the output would be produced as its fault tolerant.

