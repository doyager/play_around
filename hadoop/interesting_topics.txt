
#hive vs hbase:




#spark 1.6 vs spark 2




# rdd vs dataframe vs dataset 



# groubykey vs reduce by key 

https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html

    ->Avoid GroupByKey
        Let's look at two different ways to compute word counts, one using reduceByKey and the other using groupByKey:

        val words = Array("one", "two", "two", "three", "three", "three")
        val wordPairsRDD = sc.parallelize(words).map(word => (word, 1))

        Code:
                        val wordCountsWithReduce = wordPairsRDD
                          .reduceByKey(_ + _)
                          .collect()

                        val wordCountsWithGroup = wordPairsRDD
                          .groupByKey()
                          .map(t => (t._1, t._2.sum))
                          .collect()
                  
                  
->While both of these functions will produce the correct answer, 
->the reduceByKey example works much better on a large dataset. 
That's because Spark knows it can combine output with a common key on each partition before shuffling the data.

Look at the diagram below to understand what happens with reduceByKey. Notice how pairs on the same machine with the same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled. Then the lamdba function is called again to reduce all the values from each partition to produce one final result.


->On the other hand, when calling groupByKey - all the key-value pairs are shuffled around. 
This is a lot of unnessary data to being transferred over the network.

To determine which machine to shuffle a pair to, Spark calls a partitioning function on the key of the pair. Spark spills data to disk when there is more data shuffled onto a single executor machine than can fit in memory. However, it flushes out the data to disk one key at a time - so if a single key has more key-value pairs than can fit in memory, an out of memory exception occurs. This will be more gracefully handled in a later release of Spark so the job can still proceed, but should still be avoided - when Spark needs to spill to disk, performance is severely impacted.




# sort by vs distribute by vs cluster by vs order by 


Sort by :
                        Hive uses the columns in SORT BY to sort the rows before feeding the rows to a reducer. The sort order will be dependent on the column types. If the column is of numeric type, then the sort order is also in numeric order. If the column is of string type, then the sort order will be lexicographical order.

                        SELECT emp_id, emp_salary FROM employees SORT BY emp_salary DESC;

                        Lets assume the number of reducers were set to 2 and output of each reducer is as follows –



                        Reducer 1 :

                        emp_id | emp_salary
                        10             5000
                        16             3000
                        13             2600
                        19             1800
                        Reducer 2:
                        emp_id | emp_salary
                        11             4000
                        17             3100
                        14             2500
                        20             2000


Order by :

                        ORDER BY
                        This is similar to ORDER BY in SQL Language.

                        In Hive, ORDER BY guarantees total ordering of data, but for that it has to be passed on to a single reducer, which is normally performance intensive and therefore in strict mode, hive makes it compulsory to use LIMIT with ORDER BY so that reducer doesn’t get overburdened.

                        Ordering : Total Ordered data.

                        Outcome : Single output i.e. fully ordered.

                        For example :
                         SELECT emp_id, emp_salary FROM employees ORDER BY emp_salary DESC;


                        Reducer :

                        Shell
                        emp_id | emp_salary
                        10             5000
                        11             4000
                        17             3100
                        16             3000
                        13             2600
                        14             2500
                        20             2000
                        19             1800

DISTRIBUTE BY:
                        Hive uses the columns in Distribute By to distribute the rows among reducers. All rows with the same Distribute By columns will go to the same reducer.

                        
                        It ensures each of N reducers gets non-overlapping ranges of column, but doesn’t sort the output of each reducer. You end up with N or more unsorted files with non-overlapping ranges.

                        Example ( taken directly from Hive wiki ):-

                        We are Distributing By x on the following 5 rows to 2 reducer:

                        x1
                        x2
                        x4
                        x3
                        x1


                        Reducer 1
                        x1
                        x2
                        x1
                        Reducer 2
                        Shell
                        x4
                        x3
                        
CLUSTER BY
            Cluster By is a short-cut for both Distribute By and Sort By.

            CLUSTER BY x ensures each of N reducers gets non-overlapping ranges, then sorts by those ranges at the reducers.

            Ordering : Global ordering between multiple reducers.

            Outcome : N or more sorted files with non-overlapping ranges.

            For the same example as above , if we use Cluster By x, the two reducers will further sort rows on x:

            Reducer 1 :
            x1
            x1
            x2

            Reducer 2 :
            x3
            x4
            1

            Instead of specifying Cluster By, the user can specify Distribute By and Sort By, so the partition columns and sort columns can be different.







# fair and capacity schedulers

Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an equal share of
resources over time. When there is a single job running, that job uses the entire cluster. When other jobs are 
submitted, tasks slots that free up are assigned to the new jobs, so that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. It is also a reasonable way to share a cluster between a number of users. Finally, fair sharing can also work with job priorities - the priorities are used as weights to determine the fraction of total compute time that each job should get.

The CapacityScheduler is designed to allow sharing a large cluster while giving each organization a minimum 
capacity guarantee. The central idea is that the available resources in the Hadoop Map-Reduce cluster are partitioned 
among multiple organizations who collectively fund the cluster based on computing needs. There is an added benefit that
an organization can access any excess capacity no being used by others. This provides elasticity for the organizations 
in a cost-effective manner.

#vcores:

            -
               A VCPU is a core. Your CPU, if Hyperthreaded, doubles your amount of physical cores. 

               Example: You a Quad Core Xeon Processor Socket. It has 4 cores, but it is presented as 8 cores because of hyperthreading.
               
               lscpu
 
Thread(s) per core:    2


            -
                  VCores are virtual machine cores in the Hadoop cluster which is required to process your task (or simply an ability of CPU to compute the job in the cluster). There are some set of configurations which require these virtual cores to be set for all default applications as follows:

                  mapreduce.map.cpu.vcores - The number of virtual cores required for each map task.

                  mapreduce.reduce.cpu.vcores - The number of virtual cores required for each reduce task.

            -
                     “Core” in context of multithreading is a CPU concept - how processes can be run on system in tandem. This can be physical threads of execution added physical power by multiple cores on board and could be coupled with Hyperthreading (Intel)/CoolThreads(Solaris) - with virtual light weight threads on top of physical cores which share hardware resources giving performance boost by better management of logical cores.

                     Abstraction is moved further into application space using Java Multithreading capabilities into Hadoop/YARN in form of vCores for improving resource utilization -

                     Hadoop it can be used to configure threads at Mapper/Reduce job level in mapred-site.xml, where as
                     YARN it is more to manage maximum/minimum threads being used by containers generally by rule executor-cores*num-executors +1 in yarn-site.xml



# Hadoop is OLAP or OLTP?
 Hadoop is OLAP!!!
 
            - Since we use Hadoop to process the analysis of big data & this is done by batch wise on historical data which is loaded in the HDFS( Hadoop distributed file system). Hadoop doesn't provide any random access to the data stored in it's file. So we can't use Hadoop as an OLTP database which is characterized by INSERT -UPDATE- DELETE. hadoop provides access to historical data to carry out an analysis.

            Hence, we can conclude that hadoop is purely an OLAP (online analytical processing).

            The OLAP system does not contain present data but it contains historical or old data. OLAP data is being analyzed in order to generate reports.



             OLAP & OLTP are the techniques of data Warehousing.

            OLTP which is Online transaction processing and

            OLAP which is Online Analytical Processing .

            The difference between both is that OLAP is the reporting engine while OLTP is purely a business process engine.

            To give you a clear idea about both, I would like to give you an example which explains OLAP & OLTP in Layman terms.

            When you go to any big retail store like big Bazaar, Hypercity and after shopping is done you go to payment counter. When the receipt is printed for your name. We think only one transaction happens there that is give and take.

            well , that wasn't the only task happened there. If we had to list down the tasks they would be

            Invoice or receipt is generated.
            Storage of this data in the database.
            Items which you purchased were inserted in the database against invoice number.
            If payement is done by credit card , loyalty points are automatically credited on the card number .
            Stock of those items you purchased automatically reduced from inventory.
            If stock became below desided level , probably auto ordered to the vendor or from the warehouse (most big retail stores has this auto ordering system)
            Order number is generated & tagged against the order number.
            And so many advance task you can add to this list which were triggered at the back end. These tasks are called transactions and the system that is used for processing is called OLTP (online transaction processing)

            Now let's assume there would be 10 transactions for each customer & roughly they get 500 customers daily so total would be 500*10 = 5000 transactions daily for one store. Assuming there are 10 stores , 5000* 10 = 50000 will be the number. At national level this number will go to huge amount. And this is for one day. If we multiply by 30 that would give monthly transactions in millions.

            That's a good number to carry out an analysis & the system which is used to process the analysis is called OLAP (online analytical processing). OLAP is used to analyze the data stored in the data warehouse. This analysis can be useful for sales ,revenue and other operations of the store.

            Since we use Hadoop to process the analysis of big data & this is done by batch wise on historical data which is loaded in the HDFS( Hadoop distributed file system). Hadoop doesn't provide any random access to the data stored in it's file. So we can't use Hadoop as an OLTP database which is characterized by INSERT -UPDATE- DELETE. hadoop provides access to historical data to carry out an analysis.

            Hence, we can conclude that hadoop is purely an OLAP (online analytical processing).



