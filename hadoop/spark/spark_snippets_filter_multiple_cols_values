
- 1.spark filter 
- 2. spark filter 
--3. order a df , spark order by column 
--4.  spark filter - null values, get rows with null value 
--5. spark window function 1
--6. spark window function with multiple order by complex order by 
--7. add cast , cast to decimal type 
-- 8. spark filter - non null values, get rwos with non null value 
--9. spark filter not null and join , filter on join key is NOT null and then join with the dataframe

-------------------------------------------------------------------------
1. //to filter on column val and check nulls , multiple conditions

df_filtered = df.filter(

//condition-1
col("ds_event_date" >="2022-10-10"

&&

//condition-2 : to pick colus with nulls values
col("dim_stragtegy").isNull

&&
//condition-3 : with 3 parts in it
( col("reference_pay").isNull 
          or 
               // not equal sign is different 
  lower(col("reference_name"))   =!="payout"
          or
          //equal to has 3 = signs
   lower(col("active_ind")) === "Y"
   
 )
 && 
 //conditio-4 : split a column on PIPE and pick zero index and compare value
  (
  split(col("employers"), "\\|")(0)      =!=.   "company_1"
  )


)

-------------------------------------------------------------------------
2. //to filter one column on mulitple values

//define list of values
val CONST_PAYOUT_FILTER_VALUES= Set("FILTER_VAL1","FILTER_VAL2")

//filter condition
df_filter = df.filter(col("col_name").isInCollection(CONST_PAYOUT_FILTER_VALUES))



-------------------------------------------------------------------------

3. //  order a df , spark order by column 

df_ordered = df.orderBy("idUser")

-------------------------------------------------------------------------

4. // get NULL values
//filter a df on a column and  pick all null value rows

// picks all records with manager id null 
df_notnull = df.filter(col("manager_id").isNull) 



-------------------------------------------------------------------------

5. // spark window function




// windows function , to get the latest pay date for each user group from a full snapshot data set 

val latestPayDateWindowSpec = Window.partitionBy($"idUser").orderBy($"payDate".desc, $"create_ds".desc)

  val dataWithLatestPayDate =
    prevsData
      .withColumn("latestRow", row_number().over(latestPayDateWindowSpec))  // adding row number for all idUser groups orderded in the descending order pay_date and date
      .filter("latestRow == 1")  // just taking row-number 1 columns ,that is latest rows 
      .drop("latestRow")   // this will drop the newly added row_number column 
      .as[UserPayDateDataFormat]   // final format of the data frame 



-------------------------------------------------------------------------

6. // spark windows funciton with compplex order by - row number - analytics function



  val EMP_ALL_DESIGNATIONS = Set(
    "SE1",
    "SE2",
    "SE3",
    "Arch1",
    "Director1",
    "Director2",
    "VP",
    "SVP","CFO","CTO","CIO","CEO"
  )

  val EMP_ALL_CHIEFS= Set(
  "CFO","CTO","CIO","CEO"
  )
  

val latestPayDateWindowSpec = {
    Window.partitionBy($"department", $"role")
      .orderBy(
        //order by column is dependent on the date range, IF date range satisfyed then consider "tsPayEmittedAt" else take col "tsPayCreatedAt"
        when($"dsEvent" >= "2019-02-01" && "dsEvent" <= "2019-06-30", $"tsPayEmittedAt")
        .otherwise($"tsPayCreatedAt").desc
        , $"tsPaymentCreditedAt".desc
        , $"create_ds".desc)
    }

    
    val dataWithAllRowsGroupdBy =
     prevsData
     //additional filters : dont consider cheif officers
     .filter($"role".isInCollection(EMP_ALL_DESIGNATIONS -- EMP_ALL_CHIEFS) )
      .withColumn("latestRow",
        row_number().over(payoutReturnsDedupeWindowSpec))


          //.filter($"role".isInCollection(Const.PAYOUT_NEGATIVE_EVENTS_SET -- Set("PAYOUT_REVERSAL_RESPONSE")) && !$"dimIdTrasancation" )

-------------------------------------------------------------------------

7. // add cast , cast to decimal type 

//used method from below class to CAST
val dfCasted = df
     .withColumn("mAmtNative", $"mAmtNative".cast(DataTypes.createDecimalType(18, 6)))
     .withColumn("mAmtUsd", $"mAmtUsd".cast(DataTypes.createDecimalType(18, 6)))
     .withColumn("mAmtUsdBeforeAdjustments", $"mAmtUsdBeforeAdjustments".cast(DataTypes.createDecimalType(18, 6)))
     .as[TransactionPayments]
     
     
     
     

package org.apache.spark.sql.types;

import java.util.*;

import org.apache.spark.annotation.InterfaceStability;

/**
 * To get/create specific data type, users should use singleton objects and factory methods
 * provided by this class.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
public class DataTypes {
//method to convert to decimal type 
public static DecimalType createDecimalType(int precision, int scale) {
    return DecimalType$.MODULE$.apply(precision, scale);
  }
  
  }


-------------------------------------------------------------------------

8. // get only non null values 


    // Filter out rows with missing  Token
    val dfFiltered = df.filter(col("Token").isNotNull)
    
  
  

-------------------------------------------------------------------------

9.    // filter on JOIN KEY is NOT NULL and THEN JOIN 
    
     val joinedDF = df1
      .filter(col("token").isNotNull)
      .joinWith(
        df2,
        df1("token") === df2("token"),
        "leftouter"
      )



-------------------------------------------------------------------------

10. // multiple join , broadcast join , leftanti join , left anti , important

// join 3 dataframes , to filter out df2 and df3 out of df1 based on the join column 
//left anti join - takes all the records in left df that dont have a join in right df 
// boradcast df2 


 val fullSnapShotDF = df1
      .join(
        broadcast(df_credit_events), // Filter for credit events
        df1("idToken") === df_credit_events("idToken"),
        "leftanti"
      )
      .join(df3, //Filter for Debug Cleanup
        df1("idEvent") === df3("idEvent")
          &&
          df1("tsEventCreatedAt") <      debugCleanUpData("tsEventCreatedAt"),
        "leftanti")
      .as[PaymentSchema]




-------------------------------------------------------------------------

8.




-------------------------------------------------------------------------

8.




-------------------------------------------------------------------------

8.





-------------------------------------------------------------------------

8.




-------------------------------------------------------------------------

8.




-------------------------------------------------------------------------

8.
