


Mean:

    from pyspark import SparkContext, Spar
    from pyspark import SparkContext, SparkConf
    from pyspark.sql import SparkSession
    
    sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))

    spark = SparkSession \
    .builder \
    .getOrCreate()
    
    rdd = sc.parallelize(range(100))
    
    cnt = rdd.count()
    
    sum_val = rdd.sum()
    
    mean = sum_val/ cnt 
    
    
Median:
   
   
        sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))

        spark = SparkSession \
        .builder \
        .getOrCreate()

        rdd = sc.parallelize([102]+ range(100)). //we are adding a random value to the list to check if sorting is working

        rdd.sortBy(lambda x : x ).collect // just to sort

        //zipWithIndex will add index , but the index is added as second elemtn in the set , so we are using map to make index as the 
        // first element
        sortedAndIndexed = rdd.sortBy(lambda x : x ).zipWithIndex().map(lambda (value,key) : (key,value))
        n = sortedAndIndexed.count()

        if(n %2 ==1):
          print "odd case"
          index = (n-1)/2
          print "median :"
          print sortAndIndexed.lookup(index)
         else:
           print "even case"
           index1 = (n-1)/2
           index2 = n/2
           val1 = sortAndIndexed.lookup(index1)
           val2 = sortAndIndexed.lookup(index2)
           print "median :"
           print (val1+val2)/2

   
Standard Deviation:
   
           Standard deviation tells you how wide the data is spread around the mean. So if the standard deviation is low, then all the values are very close to the mean value,

           Stand deviation = Squre root of [ 1/N * square( n - mean) ]

           sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))

            spark = SparkSession \
            .builder \
            .getOrCreate()

            rdd = sc.parallelize(range(100))

            cnt = rdd.count()

            sum_val = rdd.sum()

            mean = sum_val/ float(cnt)    //to avoid loss of precision , we are using float 

            from math import sqrt
            sd = sqrt(rdd.map(lambda x : pow(x-mean,2)).sum()/n) //standard deviation
            print(sd)
   
   
 Skewness :
 
        Skewness basically tells you how asymmetric the data is spread around the mean.
        In this case, we see a right tail on the right hand side, so we talk about positive skew. The measure does tell us         about the asymmetry but not about a shape. So we can't distinguish between a long and flat tail, or a short and             thick tail using this measure.

        If the tail is on the left hand side, we talk about negative skew.Note that incase we have a long tailed, thin left         skew, and a short tailed, thick right skew, they might cancel out if you obtain a value close to zero for the               skewness measure.So it is always worth it to plug your data in addition to calculate statistical moments.
        
        Formula : 
        
        Summation of 1/N * ( cube of (Xj - mean) / Cube of standar deviation )
        
        
        
                Xj are the individual values. X double bar is the mean we've previously calculated. Note that we have to take                   it to the power of three. N is number of elemetns

                The same holds for the standard deviation, which we've already computed and also have to take it to the power                   of three.


            sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))

            spark = SparkSession \
            .builder \
            .getOrCreate()

            rdd = sc.parallelize(range(100))
            
            // for positive skew value ,we are adding value -1000 for 10K times 
            //.        rdd = sc.parallelize(range(100)+[-1000]*10000) 
            // skewness  -> 9.92
            
           
            // for negative skew value we are adding value 1000 for 10,000 times ,
            //.    rdd = sc.parallelize(range(100)+ [1000]*10000) 
            //.     skewness -> -9.897818

            cnt = rdd.count()

            sum_val = rdd.sum()

            mean = sum_val/ float(cnt)    //to avoid loss of precision , we are using float 

            from math import sqrt
            sd = sqrt(rdd.map(lambda x : pow(x-mean,2)).sum()/n) //standard deviation
            
            n = float(n)
            skewness = rdd.map(lambda x : pow(x-mean,3)/pow(sd,3)).sum()
            // 0.053589 for range(100)
          
  Kurtosis
  
          Kurtosis basically tells you something about the shape of your data when plotted using a histogram.
          It tells you about the outlier content or in other words, the length of the tails of the histogram. The higher the kurtosis measure is, the more outliers are present and the longer the tails of the distribution in the histogram are
          
          Formula:
          
           Summation of 1/N * ( power of 4 of (Xj - mean) / power of 4 of  standar deviation )
           
           
           
           
                Xj are the individual values. X double bar is the mean we've previously calculated. Note that we have to take                   it to the power of three. N is number of elemetns i.e. is the normalization factor

                The same holds for the standard deviation, which we've already computed and also have to take it to the power                   of three.


            sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))

            spark = SparkSession \
            .builder \
            .getOrCreate()

            rdd = sc.parallelize(range(100))
            
            // for positive skew value ,we are adding value -1000 for 10K times 
            //.        rdd = sc.parallelize(range(100)+[-1000]*10000) 
           
            
           
            // for negative skew value we are adding value 1000 for 10,000 times ,
            //.    rdd = sc.parallelize(range(100)+ [1000]*10000) 
           

            cnt = rdd.count()

            sum_val = rdd.sum()

            mean = sum_val/ float(cnt)    //to avoid loss of precision , we are using float 

            from math import sqrt
            sd = sqrt(rdd.map(lambda x : pow(x-mean,2)).sum()/n) //standard deviation
            
            n = float(n)
            kurtosis = rdd.map(lambda x : pow(x-mean,4)/pow(sd,4)).sum()
            kurtosis
            // -99.48
            
    Covariance and Correlation:
    
    
    Now, we want to learn something about the interaction of individual columns in our data. This is where covariance matrices and correlation are kicking in.

Covariance and correlation tell us how two columns are interacting with each other. So for example, consider a table containing information about different buckets of water, and two columns called volume and weight. Most likely, if the volume increases, also the weight increases. Therefore, the information content of one column is decreasing once you know the other one.
