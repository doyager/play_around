
##########################
# . pyspark - python
###########################


# start shell
pyspark --queue root.dev_team --master yarn

# start shell in local
/bin/pyspark --master local[4] --packages com.databricks:spark-csv_2.10:1.3.0


# create df in local mode , hence we say file:// in path 

    /bin/pyspark --master local[4] --packages com.databricks:spark-csv_2.10:1.3.0
    rank_new = sqlContext.load(source="com.databricks.spark.csv", path = 'file:///home/mac/test_data/rank_store_new.csv', header = True,inferSchema = True)
    rank_new.show()

    rank_history = sqlContext.load(source="com.databricks.spark.csv", path = 'file:///home/mac/test_data/rank_store_history.csv', header = True,inferSchema = True)

    rank_new.registerTempTable("new_data")

    rank_history.registerTempTable("historic_data")


    # sample data - new delta data

    ndc,metric_dec_qty,cnt,rank,total_cnt
    4777,0.143,2226,1,2802
    0037,0.286,4566,1,4998
    4778,0.143,3353,1,4299
    0031,0.286,11382,1,12191

    #sample data - history

    ndc,metric_dec_qty,cnt,rank,total_cnt
    4777,0.143,2226,1,2802
    0037,0.286,4566,1,4998
    4778,0.143,3353,1,4299
    0037,0.286,4122,1,4542


        # upsert logic 

             updated_records = " select * from historic_data a left  join ( select ndc as ndc_new, metric_dec_qty as metric_dec_qty_new,cnt as cnt_new,rank as rank_new,total_cnt as total_cnt_new from new_data) b on (a.ndc = b.ndc_new and a.metric_dec_qty = b.metric_dec_qty_new)"
            df_updated = sqlContext.sql(updated_records)
            df_updated.show()


            df_unchanged = df_updated.where(col("ndc_new").isNull()).select("ndc", "metric_dec_qty", "cnt", "rank", "total_cnt")
            df_unchanged.show()

            # filtering unchanged and adding up cnts to update the fields
            df_updated_new = df_updated.where(col("ndc_new").isNotNull()).selectExpr('ndc',' metric_dec_qty','cnt + cnt_new as cnt_en','rank','total_cnt + total_cnt_new as total_cnt')

            new_records = " select * from historic_data a right  join ( select ndc as ndc_new, metric_dec_qty as metric_dec_qty_new,cnt as cnt_new,rank as rank_new,total_cnt as total_cnt_new from new_data) b on (a.ndc = b.ndc_new and a.metric_dec_qty = b.metric_dec_qty_new)"
            df_new = sqlContext.sql(new_records)
            df_new = df_new.where(col("ndc").isNull()).selectExpr("ndc_new as ndc", "metric_dec_qty_new as metric_dec_qty", "cnt_new as cnt", "rank_new as rank", "total_cnt_new as total_cnt")
            df_new.show()


            rank_new.show()
            df_updated_new.show()
            df_unchanged.show()
            df_new.show()


            #df_final = df_updated_new.union(df_unchanged).union(df_new)
            df_final = df_updated_new.unionAll(df_unchanged).unionAll(df_new) # spark 1.6
            df_final.show()

                
                   #final upserted /updated data with history + update_history + new_delta
                +-----------+--------------+------+----+---------+                              
                |        ndc|metric_dec_qty|cnt_en|rank|total_cnt|
                +-----------+--------------+------+----+---------+
                |  3784|         0.286|  9132|   1|     9996|
                |4777|         0.143|  4452|   1|     5604|
                |4778|         0.143|  6706|   1|     8598|
                |  0037|         0.286|  4122|   1|     4542|
                |  0031|         0.286| 11382|   1|    12191|
                +-----------+--------------+------+----+---------+





            df_final.registerTempTable("final_tbl")
            
            # rank data based on cnt value in a group (ndc)
            df_final_new = sqlContext.sql( "select * , rank() over (partition by ndc  order by cnt desc)as rank_new from final_tbl")

##########################
# . scala based
###########################



#start shell
spark-shell --queue root.dev_team --master yarn



    # sample data - new delta data

    ndc,metric_dec_qty,cnt,rank,total_cnt
    4777,0.143,2226,1,2802
    0037,0.286,4566,1,4998
    4778,0.143,3353,1,4299
    0031,0.286,11382,1,12191

    #sample data - history

    ndc,metric_dec_qty,cnt,rank,total_cnt
    4777,0.143,2226,1,2802
    0037,0.286,4566,1,4998
    4778,0.143,3353,1,4299
    0037,0.286,4122,1,4542


        # upsert logic 



                val rank_new = sqlContext.read.format("csv").option("header", "true").load("file:///home/mac/test_area/outlier_detection/metric_qty_lvl/test_data/rank_store_new.csv")
                rank_new.show()

                val rank_history = sqlContext.read.format("csv").option("header", "true").load("file:///home/mac/test_area/outlier_detection/metric_qty_lvl/test_data/rank_store_history.csv")
                rank_history.show()

                rank_new.registerTempTable("new_data")

                rank_history.registerTempTable("historic_data")

                val updated_records = " select * from historic_data a left  join ( select ndc as ndc_new, metric_dec_qty as metric_dec_qty_new,cnt as cnt_new,rank as rank_new,total_cnt as total_cnt_new from new_data) b on (a.ndc = b.ndc_new and a.metric_dec_qty = b.metric_dec_qty_new)"
                val df_updated = sqlContext.sql(updated_records)
                df_updated.show()


                val df_unchanged = df_updated.where(col("ndc_new").isNull()).select("ndc", "metric_dec_qty", "cnt", "total_cnt")
                val df_unchanged = df_updated.where(col("ndc_new").isNull()).select("ndc, metric_dec_qty, cnt, total_cnt")
                df_unchanged.show()

                # filtering unchanged and adding up cnts to update the fields
                val df_updated_new = df_updated.where(col("ndc_new").isNotNull()).selectExpr('ndc',' metric_dec_qty','cnt + cnt_new as cnt','total_cnt + total_cnt_new as total_cnt')

                new_records = " select * from historic_data a right  join ( select ndc as ndc_new, metric_dec_qty as metric_dec_qty_new,cnt as cnt_new,rank as rank_new,total_cnt as total_cnt_new from new_data) b on (a.ndc = b.ndc_new and a.metric_dec_qty = b.metric_dec_qty_new)"
                df_new = sqlContext.sql(new_records)
                df_new = df_new.where(col("ndc").isNull()).selectExpr("ndc_new as ndc", "metric_dec_qty_new as metric_dec_qty", "cnt_new as cnt", "total_cnt_new as total_cnt")
                df_new.show()


                rank_new.show()
                df_updated_new.show()
                df_unchanged.show()
                df_new.show()


                #df_final = df_updated_new.union(df_unchanged).union(df_new)
                df_final = df_updated_new.unionAll(df_unchanged).unionAll(df_new) # spark 1.6
                df_final.show()

                df_final.registerTempTable("final_tbl")

                #ranking on cnt
                df_final_new = sqlContext.sql( "select * , rank() over (partition by ndc  order by cnt desc)as rank from final_tbl")
                df_final_new.show()
