
#Window functions
org.apache.spark.sql.expressions.Window

 // PARTITION BY country ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
   Window.partitionBy("country").orderBy("date")
     .rowsBetween(Window.unboundedPreceding, Window.currentRow)

   // PARTITION BY country ORDER BY date ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING
   Window.partitionBy("country").orderBy("date").rowsBetween(-3, 3)
   
   
# Narrow Transformations . [Map , filer and data lies in single partition]
            Narrow transformations are the result of map, filter and such that is from the data from a single partition only, i.e. it is self-sustained.

            An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.

            Spark groups narrow transformations as a stage which is called pipelining.

#Wide Transformations . [ groupByKey and ReducebyKey , data lies in many partitions ]
                     Wide transformations are the result of groupByKey and reduceByKey. The data required to compute the records in a single partition may reside in many partitions of the parent RDD.


                   Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle.
                   All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute RDD shuffle, which transfers data across cluster and results in a new stage with a new set of partitions.

#zipWithIndex
         zipWithIndex(): RDD[(T, Long)]
         zipWithIndex zips this RDD[T] with its element indices.
          Caution
          If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices.
 
 
 
 # repartition vs partitionByKey

       -
                    If you run repartition(COL) you change the partitioning during calculations - you will get spark.sql.shuffle.partitions (default: 200) partitions. If you then call .write you will get one directory with many files.

                   If you run .write.partitionBy(COL) then as the result you will get as many directories as unique values in COL. This speeds up futher data reading (if you filter by partitioning column) and saves some space on storage (partitioning column is removed from data files).
       -
                Watch out: I believe the accepted answer is not quite right! I'm glad you ask this question, because the behavior of these similarly-named functions differs in important and unexpected ways that are not well documented in the official spark documentation.

               The first part of the accepted answer is correct: calling df.repartition(COL, numPartitions=k) will create a dataframe with k partitions using a hash-based partitioner. COL here defines the partitioning key--it can be a single column or a list of columns. The hash-based partitioner takes each input row's partition key, hashes it into a space of k partitions via something like partition = hash(partitionKey) % k. This guarantees that all rows with the same partition key end up in the same partition. However, rows from multiple partition keys can also end up in the same partition (when a hash collision between the partition keys occurs) and some partitions might be empty.

               In summary, the unintuitive aspects of df.repartition(COL, numPartitions=k) are that

               partitions will not strictly segregate partition keys
               some of your k partitions may be empty, whereas others may contain rows from multiple partition keys
               The behavior of df.write.partitionBy is quite different, in a way that many users won't expect. Let's say that you want your output files to be date-partitioned, and your data spans over 7 days. Let's also assume that df has 10 partitions to begin with. When you run df.write.partitionBy('day'), how many output files should you expect? The answer is 'it depends'. If each partition of your starting partitions in df contains data from each day, then the answer is 70. If each of your starting partitions in df contains data from exactly one day, then the answer is 10.

               How can we explain this behavior? When you run df.write, each of the original partitions in df is written independently. That is, each of your original 10 partitions is sub-partitioned separately on the 'day' column, and a separate file is written for each sub-partition.

               I find this behavior rather annoying and wish there were a way to do a global repartitioning when writing dataframes.


   
   
   
   
   
# client mode vs cluster mode:

                A common deployment strategy is to submit your application from a gateway machine that is physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster). In this setup, client mode is appropriate. In client mode, the driver is launched directly within the spark-submit process which acts as a client to the cluster. The input and output of the application is attached to the console. Thus, this mode is especially suitable for applications that involve the REPL (e.g. Spark shell).

                Alternatively, if your application is submitted from a machine far from the worker machines (e.g. locally on your laptop), it is common to usecluster mode to minimize network latency between the drivers and the executors. Note that cluster mode is currently not supported for Mesos clusters. Currently only YARN supports cluster mode for Python applications." -- Submitting Applications

                What I understand from this is that both strategies use the cluster to distribute tasks; the difference is where the "driver program" runs: locally with spark-submit, or, also in the cluster.

                When you should use either of them is detailed in the quote above, but I also did another thing: for big jars, I used rsync to copy them to the cluster (or even to master node) with 100 times the network speed, and then submitted from the cluster. This can be better than "cluster mode" for big jars. Note that client mode does not probably transfer the jar to the master. At that point the difference between the 2 is minimal.

                -> Probably client mode is better when the driver program is idle most of the time, to make full use of cores on the local machine and perhaps avoid transferring the jar to the master (even on loopback interface a big jar takes quite a bit of seconds). And with client mode you can transfer (rsync) the jar on any cluster node.

                -> On the other hand, if the driver is very intensive, in cpu or I/O, cluster mode may be more appropriate, to better balance the cluster (in client mode, the local machine would run both the driver and as many workers as possible, making it over loaded and making it that local tasks will be slower, making it such that the whole job may end up waiting for a couple of tasks from the local machine).

                Conclusion :
                To sum up, if I am in the same local network with the cluster, I would use the client mode and submit it from my laptop. If the cluster is far away, I would either submit locally with cluster mode, or rsync the jar to the remote cluster and submit it there, in client or cluster mode, depending on how heavy the driver program is on resources.*
                AFAIK With the driver program running in the cluster, it is less vulnerable to remote disconnects crashing the driver and the entire spark job.This is especially useful for long running jobs such as stream processing type workloads.

# Dynamic allocation

        Dynamic Allocation (of Executors)
        Dynamic Allocation (of Executors) (aka Elastic Scaling) is a Spark feature that allows for
        adding or removing Spark executors dynamically to match the workload.

        Unlike the "traditional" static allocation where a Spark application reserves CPU and 
        memory resources upfront (irrespective of how much it may eventually use), in dynamic allocation you get
        as much as needed and no more. It scales the number of executors up and down based on workload, i.e. idle executors are removed, and when there are pending tasks waiting for executors to be launched on, dynamic allocation requests them.

        Dynamic allocation is enabled using 
        spark.dynamicAllocation.enabled setting. 

        When enabled, it is assumed that the External Shuffle Service is also used (it is not by default as controlled by spark.shuffle.service.enabled property).

        ExecutorAllocationManager is responsible for dynamic allocation of executors. With dynamic allocation enabled, it is started when SparkContext is initialized.

        Dynamic allocation reports the current state using ExecutorAllocationManager metric source.
        Dynamic Allocation comes with the policy of scaling executors up and down as follows:

        Scale Up Policy 
        requests new executors when there are pending tasks and increases the number of executors exponentially since executors start slow and Spark application may need slightly more.

        Scale Down Policy
        removes executors that have been idle for spark.dynamicAllocation.executorIdleTimeout seconds.


# reparition vs coalesce

        The repartition algorithm does a full shuffle and creates new partitions with data that's
        distributed evenly. Let's create a DataFrame with the numbers from 1 to 12.

        val x = (1 to 12).toList
        val numbersDf = x.toDF("number")
        numbersDf contains 4 partitions on my machine.

        numbersDf.rdd.partitions.size // => 4
        Here is how the data is divided on the partitions:

        Partition 00000: 1, 2, 3
        Partition 00001: 4, 5, 6
        Partition 00002: 7, 8, 9
        Partition 00003: 10, 11, 12
        Let's do a full-shuffle with the repartition method and get this data on two nodes.

        val numbersDfR = numbersDf.repartition(2)
        Here is how the numbersDfR data is partitioned on my machine:

        Partition A: 1, 3, 4, 6, 7, 9, 10, 12
        Partition B: 2, 5, 8, 11

        The repartition method makes new partitions and evenly distributes the data in the new partitions
        (the data distribution is more even for larger data sets).

        #Difference between coalesce and repartition

        coalesce uses existing partitions to minimize the amount of data that's shuffled. 
        repartition creates new partitions and does a full shuffle.  coalesce results in 
        partitions with different amounts of data (sometimes partitions that have much different sizes) 
        and repartition results in roughly equal sized partitions.

        #Is coalesce or repartition faster?

        coalesce may run faster than repartition, but unequal sized partitions are 
        generally slower to work with than equal sized partitions. You'll usually need to
        repartition datasets after filtering a large data set. I've found repartition 
        to be faster overall because Spark is built to work with equal sized partitions.
