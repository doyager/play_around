

Spark :

How do you check spark job performance ?
What do you do if a spark job is taking time?
What is spark persist?
Diff between tranformations and actions?
How do you do spark submit in EMR
Tell me few transformations you used 
What do you do when you have large file that has to be available in every worker node?
How do you calculate best values for spark submit options , like executor memoy, core etc?

Explain flat map in spark ? 
Diff btw map and flat map?


Spark sql – function to work on previous row ?
Get and use previous row (Lag)
LAG is a function in SQL which is used to access previous row values in current row. This is useful when we have use cases like comparison with previous value. LAG in Spark dataframes is available in Window functions
lag(Column e, int offset)
Window function: returns the value that is offset rows before the current row, and null if there is less than offset rows before the current row.


import org.apache.spark.sql.expressions.Window
//order by Salary Date to get previous salary.
//For first row we will get NULL
val window = Window.orderBy("SalaryDate")
//use lag to get previous row value for salary, 1 is the offset
val lagCol = lag(col("Salary"), 1).over(window)
myDF.withColumn("LagCol", lagCol).show()

+-----+-------+------+----------+------+
|EmpId|EmpName|Salary|SalaryDate|LagCol|
+-----+-------+------+----------+------+
| 1| John|1000.0|2016-01-01| null|
| 1| John|2000.0|2016-02-01|1000.0|
| 1| John|1000.0|2016-03-01|2000.0|
| 1| John|2000.0|2016-04-01|1000.0|
| 1| John|3000.0|2016-05-01|2000.0|
| 1| John|1000.0|2016-06-01|3000.0|
+-----+-------+------+----------+------+
Get and use next row (Lead)
LEAD is a function in SQL which is used to access next row values in current row. This is useful when we have usecases like comparison with next value. LEAD in Spark dataframes is available in Window functions
lead(Column e, int offset)
Window function: returns the value that is offset rows after the current row, and null if there is less than offset rows after the current row.


import org.apache.spark.sql.expressions.Window
//order by Salary Date to get previous salary. F
//or first row we will get NULL
val window = Window.orderBy("SalaryDate")
//use lag to get previous row value for salary, 1 is the offset
val leadCol = lead(col("Salary"), 1).over(window)
myDF.withColumn("LeadCol", leadCol).show()





+-----+-------+------+----------+-------+
|EmpId|EmpName|Salary|SalaryDate|LeadCol|
+-----+-------+------+----------+-------+
| 1| John|1000.0|2016-01-01| 1000.0|
| 1| John|1000.0|2016-03-01| 1000.0|
| 1| John|1000.0|2016-06-01| 2000.0|
| 1| John|2000.0|2016-02-01| 2000.0|
| 1| John|2000.0|2016-04-01| 3000.0|
| 1| John|3000.0|2016-05-01| null|
+-----+-------+------+----------+-------+


Spark sql : window functions 
Window functions - Sort, Lead, Lag , Rank , Trend Analysis

Tell me spark submit options ?
Tell me how do you decide on how many executors to use, how to decide on executor memory and cores?
Persist: If you have large data sets like 10G does persist works.
Performance tuning of spark. How will you increase Broadcast variable size , if you have 10G file that has to be used by all worker nodes

Diff between repartition vs coalesces 


What version control you used?\

Diff between spark and python df?
The few differences between Pandas and PySpark DataFrame are: Operation on Pyspark DataFrame run parallel on different nodes in cluster but, in case of pandas it is not possible. ... Still pandas API is more powerful than Spark. Complex operations in pandas are easier to perform than Pyspark DataFrame.

What is parquet format ? what are engine and compression used?
engine{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’
compression{‘snappy’, ‘gzip’, ‘brotli’, None}, default ‘snappy’

---
# spark difference of two data frames

df1.subtract(df2) //column names hsould be same
or
df1.join(df2,ON='join_columns', how='left_anti') // left_anti join will give everything in df1 but not in df2 

+-----+-------+------+----------+------++-----+-------+------+----------+------++-----+-------+------+----------+------++-----+-------+------+----------+------+
Python:

How do you use two version of python in same machine?
How do you do dependency management in python?
How do install particular version of pandas?
If we have old version of python then how will you run the Python 2 and python 3 versions in the same terminal.

Diff between spark and python df?
The few differences between Pandas and PySpark DataFrame are: Operation on Pyspark DataFrame run parallel on different nodes in cluster but, in case of pandas it is not possible. ... Still pandas API is more powerful than Spark. Complex operations in pandas are easier to perform than Pyspark DataFrame.

shallow copy vs deep copy?
A shallow copy constructs a new compound
 object and then (to the extent possible) 
inserts references into it to the objects
 found in the original. A deep copy
 constructs a new compound object and then,
 recursively, inserts copies into it of the objects
 found in the original.
 
 
 -   Difference b/w def function and class function[i.e. method] in python
     # function vs method 
       Method is called by its name, but it is associated to an object (dependent)
      Function is block of code that is also called by its name. (independent)
      
     Here, key differences between Method and Function in Python are explained. Java is also an OOP language, but their is no concept of Function in it. But Python has both concept of Method and Function.

                          Python Method

                          Method is called by its name, but it is associated to an object (dependent).
                          A method is implicitly passed the object on which it is invoked.
                          It may or may not return any data.
                          A method can operate on the data (instance variables) that is contained by the corresponding class
                          Basic Method Structure in Python :

                          filter_none
                          brightness_4
                          # Basic Python method  
                          class class_name 
                              def method_name () : 
                                  ...... 
                                  # method body 
                                  ......    
                          Python 3 User-Defined Method :

                          filter_none
                          edit
                          play_arrow

                          brightness_4
                          # Python 3  User-Defined  Method 
                          class ABC : 
                              def method_abc (self): 
                                  print("I am in method_abc of ABC class. ") 

                          class_ref = ABC() # object of ABC class 
                          class_ref.method_abc() 


                             Functions

                             Function is block of code that is also called by its name. (independent)
                             The function can have different parameters or may not have any at all. If any data (parameters) are passed, they are passed explicitly.
                             It may or may not return any data.
                             Function does not deal with Class and its instance concept.
                             Basic Function Structure in Python :

                             filter_none
                             brightness_4
                             def function_name ( arg1, arg2, ...) : 
                                 ...... 
                                 # function body 
                                 ......    

                             Python 3 User-Defined Function :

                             filter_none
                             edit
                             play_arrow

                             brightness_4
                             def Subtract (a, b): 
                                 return (a-b) 

                             print( Subtract(10, 12) ) # prints -2 

                             print( Subtract(15, 6) ) # prints 9 
                             Output:

                             -2
                             9



If we don’t have internet how do you test the python script that has to be deployed to AWS or S3

How do you set up CONDA environment ?commands to do that
Python unit testing best practices?
Python – diff between list and tuple ?

+-----+-------+------+----------+------++-----+-------+------+----------+------++-----+-------+------+----------+------++-----+-------+------+----------+------+
AWS:

What is EMR?
how to do spark submit in EMR?
where do you store code for spark in EMR?
How do you manage python dependencies in EMR?
Copy to S3?
S3 to redshift?




