


#####################
Hive installation
#####################

Hive Setup on Mac :

Link : https://cwiki.apache.org/confluence/display/Hive/GettingStarted


-download :

http://apache.claz.org/hive/hive-2.3.4/

-extract :

tar -xzvf hive-x.y.z.tar.gz

- cd into dir
cd /Users/mac/workspace/softwares/installed/hive/apache-hive-2.3.4-bin

- add HIVE_HOME to .bash_profile
export HIVE_HOME=/Users/mac/workspace/softwares/installed/hive/apache-hive-2.3.4-bin

-Finally, add $HIVE_HOME/bin to your PATH:

  $ export PATH=$HIVE_HOME/bin:$PATH

- to run hive , we need hadoop installed


Hive uses Hadoop, so:

you must have Hadoop in your path OR
export HADOOP_HOME=<hadoop-install-dir>
In addition, you must use below HDFS commands to create /tmp and /user/hive/warehouse (aka hive.metastore.warehouse.dir) and set them chmod g+w before you can create a table in Hive.

  $ $HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
  $ $HADOOP_HOME/bin/hadoop fs -mkdir       /user/hive/warehouse
  $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
  $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse



#####################
Hadoop installation
#####################

Link  : https://datashark.academy/how-to-setup-apache-hadoop-cluster-on-a-mac-or-linux-computer/


 - Next we need to generate secured public and private keys for remote logins as follows:

    In your terminal, type following command

    $> ssh-keygen –t rsa –P ‘’
    This will generate keys under /home/<user>/.ssh folder.

    To let hadoop access remote nodes, we need to add public keys. In our case, since its 
    same machine, so we will just copy public key to the list of authorized keys as

-  copy authorized keys 
      $> cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
      

  - Now you should be able to connect to server without it prompting for credentials. 
        Type following in the terminal to test the connectivity
      $> ssh localhost
      Last login: Jan 31 21:40:28 2018


    - install hadoop
      Link  : https://datashark.academy/how-to-setup-apache-hadoop-cluster-on-a-mac-or-linux-computer/

       - download

         https://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/

      -  extract
         tar -xzvf hadoop-2.7.3.tar.gz

      - update .bash_profile

      export HADOOP_HOME=/Users/mac/workspace/softwares/installed/hadoop/hadoop-2.7.3

      export HADOOP_MAPRED_HOME=$HADOOP_HOME

      export HADOOP_COMMON_HOME=$HADOOP_HOME

      export HADOOP_HDFS_HOME=$HADOOP_HOME

      export YARN_HOME=$HADOOP_HOME

      export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native

      export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib”

      export PATH=$HADOOP_HOME/bin:$PATH

      export PATH=$HADOOP_HOME/sbin:$PATH

    - source bash profile
      source ~/.bash_profile


    - update hadoop-env.sh , update java_home value 

       vi hadoop-env.sh

       export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home


    - core site xml


    Next open core-site.xml file and change it as follows

            <configuration>

               <property>

                   <name>hadoop.tmp.dir</name>

                   <value>/usr/local/hdfs/tmp</value>

                   <description>base for other temp directories</description>

               </property>

               <property>

                   <name>fs.default.name</name>

                   <value>hdfs://localhost:9000</value>

               </property>

            </configuration>

    - hdfs site xml


              Now open hdfs-site.xml in terminal and change it as

          $> cd $HADOOP/etc/hadoop

          $> vi hdfs-site.xml

          <configuration>

               <property>

                       <name>dfs.replication</name>

                       <value>1</value>

               </property>

          </configuration>


- configure yarn site xml

Next open yarn-site.xml file in terminal and configure it as

$> vi yarn-site.xml

<configuration>

<property>

       <name>yarn.nodemanager.aux-services</name>

       <value>mapreduce_shuffle</value>

</property>

<property>

       <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>

       <value>org.apache.hadoop.mapred.ShuffleHandler</value>

</property>

</configuration>


- mapred-site xml

Now open mapred-site.xml file and configure it as follows

$> vi mapred-site.xml

<configuration>

   <property>

       <name>mapred.job.tracker</name>

       <value>yarn</value>

   </property>

   <property>

       <name>mapreduce.framework.name</name>

       <value>yarn</value>

   </property>

</configuration>


- format hadoop namenode

        Before we can start the daemon, we must format the NameNode so it starts fresh. Let’s do it as

        $> hadoop namenode –format

-  start dfs

    Now start the distributed file system daemons as
    cd /Users/mac/workspace/softwares/installed/hadoop/hadoop-2.7.3/etc/hadoop
    $> start-dfs.sh   # starts NameNode, DataNode & SecondaryNameNode

- start YARN

      then YARN daemon as
       cd /Users/mac/workspace/softwares/installed/hadoop/hadoop-2.7.3/etc/hadoop
      $> start-yarn.sh # starts ResourceManager & NodeManager

- check all running processes



          In few minutes, it will bring all processes UP and Running which we can confirm using jps command

          $> jps –lm

          5715 org.apache.hadoop.hdfs.server.namenode.NameNode

          5799 org.apache.hadoop.hdfs.server.datanode.DataNode

          5913 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode

          6136 org.apache.hadoop.yarn.server.nodemanager.NodeManager

          6042 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager

          If you have reached this far and output of jps command is similar to as shown above, then you have done it.
          You have successfully setup Apache Hadoop Cluster on a Mac or Linux Machine. Congratulations!!!
